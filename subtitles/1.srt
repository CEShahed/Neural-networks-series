1
00:00:04,262 --> 00:00:05,762
این عدد سه هست

2
00:00:05,730 --> 00:00:10,830
این عدد به صورت نا مرتب نوشته شده و در رزولوشن خیلی پایین و در ابعاد  28 در 28 پیکسل نمایش داده شده

3
00:00:10,830 --> 00:00:13,830
اما مغز شما هیچ مشکلی برای شناسایی اون به عنوان عدد سه نداره

4
00:00:13,830 --> 00:00:16,230
من از شما میخوام که یک لحظه به اون توجه کنید!

5
00:00:16,230 --> 00:00:19,530
چقدر جالب هست که مغزها میتونن این کار رو بدون زحمتی انجام بدن!

6
00:00:19,530 --> 00:00:23,030
منظورم این هست که این، این و این هم به عنوان عدد سه قابل شناسایی هستند

7
00:00:23,030 --> 00:00:28,530
حتی اگر مقادیر خاص هر پیکسل در یک تصویر با تصویر بعدی خیلی متفاوت باشه

8
00:00:28,530 --> 00:00:33,530
وقتی این "سه" رو میبینید، سلول های حساس به نورِ خاصی تو چشم شما فعال میشن

9
00:00:33,530 --> 00:00:37,030
با سلول هایی که فعال میشن وقتی که این "سه" رو میبینید، خیلی متفاوت هستن

10
00:00:37,030 --> 00:00:40,929
اما چیزی که در لایه بینایی هوشمند شما وجود داره

11
00:00:40,929 --> 00:00:43,829
اینها رو به عنوان نمایشی از همون ایده (عدد "سه" اول) تحلیل میکنه

12
00:00:43,862 --> 00:00:48,661
در حالی که در همون زمان ، تصاویر دیگه رو به عنوان ایده های متمایز دیگه تشخیص میده

13
00:00:48,929 --> 00:00:56,429
اما اگر من به شما بگم، یک برنامه برام بنویس که ی جدول  28 پبکسل در 28 پیکسلی مثل این رو دریافت کنه

14
00:00:56,429 --> 00:01:02,228
و خروجیش یک عدد بین 0 تا 10 باشه و به شما بگه که این چه عددی هست

15
00:01:02,228 --> 00:01:06,230
خب این کار، از یک کار مضحک بیهوده به یک کار سخت تبدیل میشه

16
00:01:06,230 --> 00:01:08,730
مگر اینکه زیر سنگ زندگی کرده باشین

17
00:01:08,730 --> 00:01:15,030
فکر می کنم که باید ارتباط و اهمیت یادگیری ماشین و شبکه های عصبی،  برای حال و آیندگان توضیح بدم

18
00:01:15,030 --> 00:01:19,950
اما اون چیزی که من اینجا میخوام به شما نشون بدم این هست که شبکه عصبی در واقع چیه - با فرض این که هیچ پیش زمینه ندارید

19
00:01:19,950 --> 00:01:22,128
و با به تصویر کشیدنش بهتون کمک کنم

20
00:01:22,128 --> 00:01:24,629
نه به عنوان یک کلیدواژه بلکه به عنوان یک تکه ریاضی

21
00:01:24,629 --> 00:01:28,828
امیدوارم که شما احساس کنید این ساختار از چیزی انگیزه گرفته شده

22
00:01:28,828 --> 00:01:35,328
و وقتی که شما درباره یادگیری مستقیم یا غیر مستقیم یک شبکه عصبی می خونید یا میشنوید؛ احساس کنید میدونید معنای اون چیه

23
00:01:35,328 --> 00:01:40,328
این ویدیو فقط به ساختار بخشی از اون اختصاص داده شده؛ و بعدی به یادگیری میپردازه

24
00:01:40,328 --> 00:01:46,328
اون چیزی که ما انجام میدیم، کنار هم قرار دادن شبکه عصبی ای هست که میتونه یاد بگیره اعداد دست نوشته رو تشخیص بده

25
00:01:49,361 --> 00:01:52,662
این یک مثال تقریبا قدیمی برای معرفی موضوع هست

26
00:01:52,662 --> 00:01:57,962
و من بخاطر وضعیت موجود خوشجالم؛ چون در پایان دو ویدیو میخوام چند منبع خوب به شما معرفی کنم

27
00:01:57,962 --> 00:02:01,563
که در اونها میتونید اطلاعات بیشتری کسب کنید و کدی که این کارو انجام میده دانلود کنید

28
00:02:01,563 --> 00:02:03,361
و روی کامپیوتر خودتون با اون باهاش کار کنید

29
00:02:04,962 --> 00:02:07,962
انواع خیلی خیلی گوناگونی از شبکه های عصبی وجود داره

30
00:02:07,962 --> 00:02:12,263
و در سال های اخیر تحقیقات زیادی درباره این گونه ها صورت گرفته

31
00:02:12,263 --> 00:02:19,062
اما تو این دو ویدیوی مقدماتی، ما فقط میخوایم ساده ترین شکل اون رو، بدون هیچ مورد غیر ضروری ای، بررسی کنیم

32
00:02:19,062 --> 00:02:24,360
این به نوعی یک پیش نیاز و شرط لازم برای درک و فهم هر یک از انواع مدرن و قدرتمند تر هست

33
00:02:24,360 --> 00:02:28,360
به نظر من، هنوز هم پیچیدگی های زیادی برای ما داره، تا ذهنمون بتونه بر اون احاطه پیدا کنه

34
00:02:28,360 --> 00:02:33,163
اما حتی تو این ساده ترین نوع هم (این شبکه) میتونه یاد بگیره که ارقام دست نویس رو تشخیص بده

35
00:02:33,163 --> 00:02:36,163
که برای یک کامپیوتر کار خیلی جالبی هست که بتونه انجام بده

36
00:02:37,360 --> 00:02:42,163
و در عین حال شما میبینید که چجوری از امیدهای اندکی که ممکنه داشته باشه، کم میشه

37
00:02:43,193 --> 00:02:46,996
همونطور که از اسمش پیداست، شبکه های عصبی از مغز الهام گرفته اند

38
00:02:47,161 --> 00:02:48,758
اما اجازه بدین که اون رو به طور ریزتر بررسی کنیم

39
00:02:48,794 --> 00:02:51,996
عصب ها چی هستن و ارتباط اونها با هم به چه معناست؟

40
00:02:51,996 --> 00:02:58,096
در حال حاضر وقتی می گم "عصب" ، تنها چیزی که می خوام به اون فکر کنید، چیزی هست که یک عدد رو در خودش جای داده

41
00:02:58,096 --> 00:03:02,596
به طور مشخص عددی بین "1" و "0" ؛ و واقعا هم بیشتر از این نیست

42
00:03:03,735 --> 00:03:04,532
به عنوان مثال

43
00:03:04,496 --> 00:03:11,294
شبکه توسط یک دسته از عصب های متناظر بهم، که مربوط به 28 پیکسل در 28 پیکسل تصویر ورودی هستن، شروع میشه

44
00:03:11,294 --> 00:03:14,395
که در کل، در اون تعداد 784 عصب وجود دارن

45
00:03:14,395 --> 00:03:16,496
که هر کدوم از این اعداد نگه داری شده

46
00:03:16,496 --> 00:03:20,294
و نشون دهنده عددی متناطر با اون رنگ سیاه و سفید پیکسلش هست

47
00:03:20,294 --> 00:03:24,693
که دامنه اون عدد از "0" برای پیکسل های سیاه تا "1" برای پیکسل های سفید متغیر هست

48
00:03:25,092 --> 00:03:28,395
به این عدد توی عصب، میگن عدد "فعالسازی" ا

49
00:03:28,894 --> 00:03:30,395
و تصویری که در اینجا ممکنه تو ذهنتون داشته باشین اینطوری هست که

50
00:03:30,395 --> 00:03:34,496
هر عصب زمانی روشن می شه که عدد فعال شدگی اش بالا باشه

51
00:03:36,496 --> 00:03:41,794
بنابراین تمام این 784 عصب اولین لایه شبکه مارو تشکیل میدن

52
00:03:45,895 --> 00:03:47,996
حالا به آخرین لایه میریم

53
00:03:47,996 --> 00:03:51,496
این لایه ده عصب داره که هر کدوم از اونها یک رقم رو نشون میده

54
00:03:51,496 --> 00:03:56,496
عدد فعالسازی توی این عصب ها هم عددی بین "0" و "1" هست

55
00:03:56,496 --> 00:04:02,193
که نشون میده سیستم چقدر فکر میکنه که یک تصویر داده شده، با رقم معین داده شده مطابقت داره

56
00:04:02,193 --> 00:04:06,395
همچنین لایه های چندگانه ای در این بین وجود داره؛ که به اونها لایه های پنهان میگن

57
00:04:06,395 --> 00:04:13,895
که برای الان، بزارید ی علامت سوال بمونه که چظور کار میکنه ( توی قسمت های بعدی توضیح میدیم ) ا

58
00:04:13,895 --> 00:04:17,795
در این شبکه من دو لایه مخفی رو انتخاب کردم که هرکدوم دارای 16 عصب هستن

59
00:04:17,795 --> 00:04:20,795
راستشو بخوای، این ی انتخاب دلخواه هست

60
00:04:20,795 --> 00:04:25,295
صادقانه بگم؛ من دو لایه روبر اساس اینکه چطور میخوام رفتار کنه انتخاب کردم

61
00:04:25,295 --> 00:04:28,595
و خب "16" ، عدد خوبیه ؛ خب خاطر این تعداد که توی صفحه خوب جا میگیره

62
00:04:28,596 --> 00:04:32,394
در عمل، فضای زیادی برای تغییر این ساختار وجود داره

63
00:04:32,995 --> 00:04:38,295
فعالسازی عصب ها در یک لایه، فعالسازی عصب های لایه بعدی رو مشخص میکنه

64
00:04:38,696 --> 00:04:43,095
و البته قلب یک شبکه، به عنوان یک سیستم پردازش اطلاعات

65
00:04:43,095 --> 00:04:49,194
به این صورت خلاصه میشه که چطوری اون فعالسازی ها در یک لایه، باعث فعالسازی در لایه بعدی میشن

66
00:04:49,196 --> 00:04:53,766
ی جورایی معادل جوری هست که عصب های طبیعی کار میکنن

67
00:04:53,766 --> 00:04:57,538
فعال شدن بعضی از عصب ها، باعث فعال شدن عصب های دیگه میشه

68
00:04:57,596 --> 00:05:01,795
و حالا شبکه ای که من اینجا نشون میدم، قبلا برای تشخیص ارقام تمرین داده شده

69
00:05:01,795 --> 00:05:03,495
اجازه بدین بهتون نشون بدم که منظورم چیه

70
00:05:03,495 --> 00:05:12,495
این به این معناست که اگر یک تصویر روی به عنوان ورودی بهش بدی، همه عصب های لایه ورودی رو مطابق با روشنایی هر 784 پیکسل در تصویر روشن میکنید

71
00:05:12,495 --> 00:05:17,295
اون الگوی فعالسازی، باعث ایجاد یک الگوی خیلی خاص در لایه بعدی میشه

72
00:05:17,295 --> 00:05:19,694
که اون هم باعث ایجاد الگویی در یک لایه بعد از خودش میشه

73
00:05:19,696 --> 00:05:22,196
که در نهایت، الگویی رو در لایه خروجی ارائه میده

74
00:05:22,196 --> 00:05:26,995
و روشن ترین عصب های اون لایه خروجی، انتخاب شبکه هست

75
00:05:26,995 --> 00:05:30,394
که یعنی این تصویر چ عددی رو نشون میده

76
00:05:32,194 --> 00:05:37,396
و قبل از اینکه بریم سراغ ریاضیات، برای فهمیدن اینکه یک لایه چطور روی لایه بعد اثر میزاره یا تمرین دادن چطوری انجام میشه

77
00:05:37,396 --> 00:05:38,694
بیاید درباره این صحبت کنیم که

78
00:05:38,696 --> 00:05:43,696
چرا منطقیه که انتظار داشته باشیم، ساختار لایه ای مثل این هوشمندانه رفتار کنه

79
00:05:43,696 --> 00:05:45,196
ما اینجا چه انتظاری داریم؟

80
00:05:45,196 --> 00:05:48,696
بهترین انتظار برای کاری اون لایه های میانی ممکنه انجام بدن چیه؟

81
00:05:48,696 --> 00:05:53,894
وقتی من یا شما ارقام رو تشخیص میدیم؛ (در واقع) ما اجزای مختلف رو با هم ترکیب میکنیم

82
00:05:53,896 --> 00:05:57,194
عدد "9" یک حلقه در بالا و یک خط در سمت راست داره

83
00:05:57,196 --> 00:06:01,196
همچنین عدد "8" هم یک حلقه در بالا و یک حلقه در پایین داره

84
00:06:01,896 --> 00:06:06,696
عدد "4" هم به سه خط خاص و مواردی مثل این تقسیم میشه

85
00:06:06,696 --> 00:06:09,995
حالا در یک دنیای ایده آل، ممکنه بتونیم امیدوار باشیم که

86
00:06:09,995 --> 00:06:14,795
هر عصب از لایه دوم تا لایه آخر با یکی از این مولفه های فرعی مطابقت داشته باشه

87
00:06:14,795 --> 00:06:19,995
که هر زمان که شما یک تصویر با یک حلقه در بالا ، مثل عدد "9" یا "8" بهش میدید

88
00:06:19,995 --> 00:06:24,095
عصب خاصی وجود داره که فعالسازی اون نزدیک به "1" هست

89
00:06:24,096 --> 00:06:26,596
منظور من این حلقه خاص از پیکسل ها نیست

90
00:06:26,696 --> 00:06:31,934
ما امیدواریم که هر الگویی که دارای یک حلقه به سمت بالا باشه، این عصب (عصب خاص) رو فعال کنه

91
00:06:31,934 --> 00:06:34,934
از لایه سوم به سمت لایه آخر

92
00:06:35,096 --> 00:06:40,295
فقط نیاز هست بدونید که کدوم یک از اجزای فرعی با کدوم رقم مطابقت داره

93
00:06:40,694 --> 00:06:43,095
البته این کار، مسئله های بیشتری رو مطرح میکنه

94
00:06:43,096 --> 00:06:45,396
چون چطوری میتونید این مولفه های فرعی رو تشخیص بدین

95
00:06:45,396 --> 00:06:47,896
و یا حتی مولفه های فرعی مناسب چی هستن؟

96
00:06:47,896 --> 00:06:50,995
و من حتی درباره اینکه یک لایه روی لایه بعدی اثر میزاره هم هنوز صحبت نکردم

97
00:06:51,096 --> 00:06:52,908
ولی چند لحظه باهام همراه باشید

98
00:06:53,139 --> 00:06:56,995
تشخیص یک حلقه میتونه، به اجزای فرعی تبدیل بشه

99
00:06:56,995 --> 00:06:58,995
یک راه مناسب برای انجام این کار، این هست که

100
00:06:58,995 --> 00:07:02,894
ابتدا لبه های (مرزهای) کوچک و مختلفی که اون حلقه رو تشکیل میدن بشناسید

101
00:07:02,896 --> 00:07:08,396
مانند یک خط طولانی که ممکنه در ارقام "1" ، "4" یا "7" ببینید

102
00:07:08,396 --> 00:07:10,894
خب این واقعا یک لبه بلند هست

103
00:07:10,896 --> 00:07:14,694
یا ممکنه شما اون رو به عنوان الگوی خاصی از چند لبه کوچکتر ببینید

104
00:07:14,696 --> 00:07:17,394
بنابراین شاید امید ما به این باشه که

105
00:07:17,396 --> 00:07:23,396
هر عصب از لایه دوم شبکه، متناظر با لبه های کوچک مختلفی باشه

106
00:07:23,396 --> 00:07:31,896
شاید وقتی تصویری مثل این داده میشه، تمام عصب های متناظر با تقریبا 8 الی 10 لبه کوچک خاص، روشن میشن

107
00:07:31,896 --> 00:07:37,396
که همه اونها عصب های مربوط به حلقه بالایی و یک خط عمودی بلند (در عدد"9") رو روشن میکنن

108
00:07:37,396 --> 00:07:40,396
و (در نهایت اینطوری)  عصب های مربوط به "9" رو فعال می کنن

109
00:07:40,396 --> 00:07:43,394
اینکه آیا این واقعا همون کاری هست که شبکه نهایی ما انجام میده یا نه

110
00:07:43,396 --> 00:07:47,396
سوال دیگه ای هست که زمانی که بفهمیم یک شبکه رو چطور تمرین بدیم، بهش بر میگردیم

111
00:07:47,396 --> 00:07:52,595
شاید بتونیم به نوعی هدف (متناسب) با ساختار لایه ای مثل این امیدوار باشیم

112
00:07:52,596 --> 00:07:54,596
علاوه بر این ؛ میتونید تصور کنید که

113
00:07:54,596 --> 00:07:57,096
چطوری توانایی تشخیص لبه ها و الگو های اینطوری

114
00:07:57,096 --> 00:08:00,596
برای دیگر کارهای تشخیصِ تصویر، واقعا مفید هستن

115
00:08:00,596 --> 00:08:05,095
و حتی فراتر از کارهای تشخیص تصویر، انواع کارهای هوشمندانه ای هست که ممکنه شما بخواید انجام بدین

116
00:08:05,096 --> 00:08:07,295
که این کارها به لایه های انتزاعی ای تجزیه میشن

117
00:08:07,295 --> 00:08:09,595
برای مثال : تجزیه گفتار

118
00:08:09,596 --> 00:08:15,096
که شامل صدای خام و استخراج صداهای متمایزی هست که با ترکیبشون "هجا" های خاصی ساخته میشه

119
00:08:15,096 --> 00:08:18,526
که باهم ترکیب میشن و کلمات رو تشکیل میدن و (کلمات) باهم ترکیب میشن و عبارات رو تشکیل میدن

120
00:08:18,526 --> 00:08:20,641
و غیره

121
00:08:20,896 --> 00:08:23,595
اما بر میگردیم به نحوه عملکرد هر کدوم از اینها

122
00:08:23,795 --> 00:08:25,995
همین الان خودتون رو در حال طراحی تصور کنید

123
00:08:25,995 --> 00:08:30,495
دقیقا چجوری فعالسازی ها توی یک لایه میتونن فعالسازی های لایه بعدی رو تعیین کنن

124
00:08:30,896 --> 00:08:33,595
هدف اینه که طرز کاری داشته باشیم که

125
00:08:33,596 --> 00:08:39,195
بتونه «پیکسل ها رو به لبه ها» یا «لبه هارو به الگو ها»  یا «الگو ها رو به ارقام» تبدیل کنه

126
00:08:39,196 --> 00:08:41,894
روی یک مثال خیلی خاص بزرگ نمایی میکنیم

127
00:08:41,894 --> 00:08:47,394
بزارید بگیم که امیدواریم که یک عصب خاص تو لایه دوم بتونه تشخیص بده که

128
00:08:47,394 --> 00:08:50,394
تصویر در این منطقه لبه داره یا نه

129
00:08:51,096 --> 00:08:52,793
سوالی که داریم اینه که

130
00:08:52,796 --> 00:08:58,394
یک شبکه چه پارامتر هایی باید داشته باشه؛ و چه شاخص هایی رو باید تغییر بده

131
00:08:58,394 --> 00:09:00,796
که به اندازه کافی رسا باشه تا براش ممکن باشه

132
00:09:00,796 --> 00:09:08,195
که این الگو یا هر الگوی پیکسلی دیگر یا الگویی که با چندین لبه می تونه یک حلقه ایجاد کنه و موارد دیگه ای مثل این رو نشون بده

133
00:09:08,340 --> 00:09:15,754
کاری که قراره بکنیم اینه که ی وزن به اتصال بین این عصب و عصب های لایه اول نسبت بدیم

134
00:09:15,754 --> 00:09:18,254
این وزن هایی که نسبت میدیم، یک عدد هستن

135
00:09:18,384 --> 00:09:25,384
بعد عدد "فعالسازی" همه عصب های لایه اول رو میگیریم و جمع وزن دارشون رو محاسبه میکنیم

136
00:09:26,831 --> 00:09:32,009
تصور کنید این وزن ها خودشون توی ی جدول قرار گرفتن

137
00:09:32,158 --> 00:09:37,532
رنگ سبز به معنی وزن مثبت و قرمز به معنی وزن منفی هست

138
00:09:37,532 --> 00:09:41,940
و روشنایی اون پیکسل، مشخص کننده قدر مطلق مقدار وزنه

139
00:09:42,091 --> 00:09:49,764
خب اگر همه وزن های مربوط به این پیکسل هارو صفر کنیم غیر از جایی که برامون مهمه

140
00:09:49,759 --> 00:09:58,259
اون موقع برای محاسبه وزن همه پیکسل ها، کافیه که فقط جمع وزن دار اون قسمت رو محاسبه کنیم

141
00:09:58,648 --> 00:10:02,355
و اگر بخوای بفهمی که آیا اون قسمت توی عکس گوشه است یا نه

142
00:10:02,355 --> 00:10:07,355
راه حلش اینه که ی سری وزن منفی دور اون قسمت داشته باشی

143
00:10:07,389 --> 00:10:13,807
وقتی جمع مثبت میشه که اون پیکسل های وسط روشن تر باشن و پیکسل های دورش تیره تر

144
00:10:14,541 --> 00:10:18,951
اگر با این روش وزن رو حساب کنی، ممکنه هر عددی بدست بیاری

145
00:10:18,980 --> 00:10:24,094
ولی برای این شبکه، ما میخوایم که عدد فعالسازی هر عصب ی چیزی بین 0 و 1 باشه

146
00:10:24,094 --> 00:10:31,855
ی روش رایج اینه که عددمون رو بدیم به ی تابع که بازه اعداد حقیقی رو به ی چیزی بین 0 و 1 تبدیل کنه

147
00:10:32,517 --> 00:10:35,662
ی تابع معروف که اینکارو میکنه، تابع "سیگموید" هستش

148
00:10:35,677 --> 00:10:37,690
که بهش "شیب منطقی" هم میگن

149
00:10:37,783 --> 00:10:43,634
به طور ساده اینطوری که خروجی ی عدد خیلی منفی، نزدیک 0 و ی خروجی ی عدد خیلی مثبت نزدیک 1 میشه

150
00:10:43,875 --> 00:10:46,941
و اطراف عدد 0 با مقدارش زیاد میشه

151
00:10:48,899 --> 00:10:56,595
پس عدد فعالسازی یک عصب، یک اندازه گیری هست که مشخص میکنه جمع وزن متناظرش چقدره

152
00:10:57,144 --> 00:11:01,964
ولی شاید تو نخوای وقتی که جمع مثبت بشه، این عصب فعال بشه

153
00:11:01,964 --> 00:11:06,768
شاید توی میخوای وقتی این عصب فعال بشه که مقدارش بیشتر از 10 بشه

154
00:11:06,879 --> 00:11:10,899
یعنی تو به ی مبنا احتیاج داری که مرز بین فعال شدن و نشدن رو مشخص کنه

155
00:11:10,899 --> 00:11:19,899
راه حلش اینه که قبل از این که حاصل رو توی تابع سیگموید بزاری، ی عدد به اون حاصل اضافه کنی، به طور مثال -10

156
00:11:20,504 --> 00:11:23,330
به این عدد، میگن مبنا

157
00:11:23,504 --> 00:11:28,687
پس وزن ها مشخص میکنن که چه الگوی پیکسلی رو توی لایه دوم دنبالشی

158
00:11:28,687 --> 00:11:35,187
و عدد مبنا مشخص میکنه که حاصل جمع باید حداقل چند باشه تا اون عصب فعال بشه

159
00:11:35,423 --> 00:11:37,860
این فقط واسه ی عصب بود

160
00:11:37,860 --> 00:11:45,182
همه عصب ها توی لایه دوم هم با همه 784 تا عصب از لایه اول اتصال دارن

161
00:11:45,182 --> 00:11:51,000
و هر کدوم از اون 784 تا اتصال، عددی به عنوان وزن دارن

162
00:11:51,451 --> 00:11:53,634
و هرکدوم عدد مبنا دارن

163
00:11:53,634 --> 00:11:57,849
ی عددی که قبل از این که جمع وزن دار رو به تابع سیگموید بدی، بهش اضافه میکنی

164
00:11:58,238 --> 00:11:59,634
اگر بهش فکر کنی میفهمی که خیلی زیاده!

165
00:11:59,985 --> 00:12:02,466
و با در نظر گرفتن این که لایه دوم 16 تا عصب داره

166
00:12:02,466 --> 00:12:08,311
ا 784*16 تا وزن و عدد مبنا وجود داره

167
00:12:08,311 --> 00:12:12,267
همه اینا تازه اتصال بین لایه اول و دوم بود

168
00:12:12,437 --> 00:12:17,614
اتصال های بین لایه های دیگه هم وزن و عدد مبنا دارن

169
00:12:18,033 --> 00:12:24,086
همه شون رو که حساب کنی، میبینی که 13002 تا ورن و عدد مبنا داره

170
00:12:24,086 --> 00:12:30,113
سیزده هزار متغیر که میشه تغییرشون داد تا این شبکه جور دیگه ای رفتار کنه

171
00:12:30,812 --> 00:12:41,812
پس وقتی میگیم "یادگیری"، منظورمون اینه که با کامپیوتر واسه این همه متغیر، ی سری عدد پیدا کنیم که بتونه مسئله رو حل کنه

172
00:12:42,196 --> 00:12:50,110
توی ی آزمایش ذهنی که به نظرم خیلی وحشتناکه، تصور کن که بشینی و همه این 13 هزار تا متغیر رو دستی تنظیم کنی

173
00:12:50,355 --> 00:12:54,177
جوری دستی این متغیر هارو دستکاری که کنی لایه دوم گوشه هارو پیدا کنه

174
00:12:54,177 --> 00:12:56,677
لایه سوم الگو هارو پیدا کنه و غیره

175
00:12:56,869 --> 00:13:02,369
به نظرم اینطوری بهتره نسبت به موقعی که به شبکه عصبی به صورت ی جعبه سیاه و جادویی نگاه کنی

176
00:13:02,475 --> 00:13:05,423
بخاطر اینکه وقتی شبکه طوری که تو پیشبینی کردی عمل نمیکنه

177
00:13:05,423 --> 00:13:09,883
اگر فهمیدی که هرکدوم از این وزن ها و اعداد مبنا چه معنی میدن

178
00:13:10,120 --> 00:13:14,581
نقطه شروع خوبی میتونه باشه برای اینکه ساختار شبکه رو طوری تغییر بدی تا اوضاع بهتر بشه

179
00:13:14,581 --> 00:13:18,163
یا وقتی که شبکه کار میکنه ولی نه به دلایلی که انتظارشو داشتی

180
00:13:18,163 --> 00:13:22,884
فهمیدن این که این عدد های پایه و وزن ها دارن چیکار میکنن، روش خوبیه تا فرض هایی که داشتی رو به چالش بکشی

181
00:13:22,884 --> 00:13:25,980
و به راه حل های مختلف پی ببری

182
00:13:25,980 --> 00:13:30,980
نوشتن کل این تابع یکم سخته، مگه نه؟

183
00:13:32,071 --> 00:13:37,460
خب بزارید ی روش جمع و جور بهتون نشون بدم که این اتصال هارو نمایش میده

184
00:13:37,567 --> 00:13:40,576
اگر در مورد شبکه عصبی مقاله بخونی، احتمالا فرمول هارو اینطوری میبینی

185
00:13:41,215 --> 00:13:46,245
اینطوری که همه عدد های "فعالسازی" یک لایه رو به صورت ی بردار بنویسی

186
00:13:47,692 --> 00:13:50,570
و همه وزن هارو به صورت ماتریس بنویسی

187
00:13:50,570 --> 00:13:58,181
که هر سطر این ماتریس مشخص کننده وزن های اتصال های بین یک "لایه" و یک "عصب" خاص از لایه بعدی هستن

188
00:13:58,181 --> 00:14:13,181
یعنی جواب جمع وزن دار لایه اول با توجه به هرکدوم از وزن های اتصال ها، میشه عدد توی سطر متناظر ماتریس حاصل

189
00:14:14,365 --> 00:14:18,365
خیلی از مباحث یادگیری ماشین به دونستن جبر خطی وابسته است

190
00:14:18,692 --> 00:14:24,192
برای کسایی که میخوان ی فهم تصویری از این که ماتریس ها چی هستن و ضرب ماتریسی چیه، داشته باشن

191
00:14:24,259 --> 00:14:28,259
به دوره جبر خطی من ی نگاه بندازید - مخصوصا فصل 3 اش

192
00:14:28,918 --> 00:14:34,918
بجای این که عدد مبنا رو به این مقدار ها دونه دونه اضافه کنیم

193
00:14:34,888 --> 00:14:42,388
اونا رو به صورت ی بردار نمایش میدیم و اون رو با حاصل ضرب ماتریسی قبلی جمع میکنیم

194
00:14:42,672 --> 00:14:47,672
و در آخر اونا رو میزاریم توی تابع سیگموید

195
00:14:47,917 --> 00:14:54,917
و این کار مثل این میمونه که تابع سیگموید رو روی تک تک اعضای ماتریس حاصل اعمال کنید

196
00:14:55,591 --> 00:15:00,567
اگر ماتریس وزن ها و بردار هارو به صورت نمادشون بنویسیم

197
00:15:00,567 --> 00:15:08,067
میتونی مقدار عددهای فعالسازی رو توی ی عبارت کوچیک نشون بدی

198
00:15:08,171 --> 00:15:12,118
این کار نوشتن کد(برنامه کامپیوتری) اش رو خیلی ساده تر و سریع تر میکنه

199
00:15:12,118 --> 00:15:15,980
از اونجایی که خیلی از کتابخونه ها، ضرب های ماتریسی پیچیده رو به صورت بهینه انجام میدن

200
00:15:17,128 --> 00:15:22,003
یادته که گفتم عصب به طور ساده، ی چیزیه که عدد داره؟

201
00:15:22,115 --> 00:15:26,754
اینم باید اضافه کنم که عددی که میگیرن بستگی داره به اینکه چه عکسی به عنوان ورودی بهش بدی

202
00:15:27,774 --> 00:15:31,778
پس بهتره هر عصب رو مثل ی تابع در نظر بگیری

203
00:15:31,778 --> 00:15:35,831
تابعی که مقدار عصب های لایه قبل رو میگیره

204
00:15:35,831 --> 00:15:38,941
و ی عدد بین 0 تا 1 خروجی میده

205
00:15:39,173 --> 00:15:41,317
درواقع، کل شبکه مثل ی تابع میمونه

206
00:15:41,317 --> 00:15:47,402
تابعی که 784 تا پیکسل رو به عنوان ورودی میگیره و در آخر 10 تا عدد خروجی میده

207
00:15:47,402 --> 00:15:53,812
ی تابع خیلی پیچیده است از اونجایی که سیزده هزار متغیر داره که به شکل وزن ها و عدد های مبنا هستن

208
00:15:53,812 --> 00:15:55,759
که هر کدوم الگو های خاصی رو مشخص میکنن

209
00:15:55,759 --> 00:16:00,571
که نیازه تا کلی عملیات ماتریسی با کمک تابع سیگموید انجام بشه

210
00:16:00,850 --> 00:16:03,557
ولی به هرحال ی تابع به حساب میاد

211
00:16:03,557 --> 00:16:06,835
با این اوضاع، مطمئنا تابع پیچیده ایه

212
00:16:06,835 --> 00:16:12,844
اگر راه ساده تری وجود داشت، چ انگیزه ای برای تشخیص عدد توسط کامپیوتر داشتیم؟

213
00:16:12,820 --> 00:16:14,966
و کامپیوتر چطور این کار چالشی رو انجام میده؟

214
00:16:14,966 --> 00:16:19,589
چطوری کامپیوتر این همه وزن و عدد مبنا رو فقط از طریق داده هایی که بهش میدیم پیدا میکنه؟

215
00:16:19,589 --> 00:16:21,846
خب اینو توی قسمت بعدی بهتون میگم

216
00:16:21,846 --> 00:16:26,711
و بهتون میگم که این شبکه دقیقا چیکار میکنه

217
00:16:26,711 --> 00:16:33,211
الان موقع خوبیه که بهتون بگم مارو دنبال کنید تا هروقت که ویدیو جدید اومد مطلع بشید

218
00:16:33,269 --> 00:16:37,269
ولی بیشتر شما از یوتیوب اعلان دریافت نمیکنید، مگه نه؟

219
00:16:37,466 --> 00:16:45,096
شاید بهتره بگم مارو دنبال کن تا شبکه مصنوعی که پشت سیستم پیشنهاد ویدئوی یوتیوب هست مطمئن بشن

220
00:16:45,096 --> 00:16:48,339
که میخوای مطالب و ویدیو های این کانال بهت پیشنهاد داده بشه

221
00:16:48,339 --> 00:16:50,339
به هرحال در جریان مطالب جدید باشید :D

222
00:16:50,533 --> 00:16:53,908
داره از کسایی که کمک مالی اش کردن تشکر میکنه

223
00:16:54,076 --> 00:17:02,076
...

224
00:17:03,445 --> 00:17:12,352
در انتها، از خانم "لیشا لی" که پایان نامه دکترا اش رو در رابطه با بخش تئوری "یادگیری عمیق" انجام داده و توی .... کار میکنه

225
00:17:12,352 --> 00:17:15,394
همونجایی که برای درست کردن این ویدئو بهم کمک مالی کردن، دعوت میکنم

226
00:17:15,394 --> 00:17:19,192
خب "لیشا" به نظرم بهتره مطرح کنیم، تابع سیگموید عه

227
00:17:19,192 --> 00:17:24,970
اونطوری که من فهمیدم، شبکه های عصبی اولیه از این تابع استفاده میکردن تا جمع وزن دار رو به ی چیزی بین 0 و 1 تبدیل کنن

228
00:17:24,970 --> 00:17:29,383
و این ی جورایی که زیست شناسی الهام گرفته شده که عصب ها فعال بشن یا نه

229
00:17:29,383 --> 00:17:30,194
- دقیقا

230
00:17:30,194 --> 00:17:35,769
ولی تعداد کمی از شبکه های عصبی جدید از این تابع استفاده میکنن و ی جورایی قدیمیه - درسته؟

231
00:17:35,769 --> 00:17:39,029
اره درسته. یا اینکه تابع "رلیو" به ساده تر به نظر میرسید برای تمرین دادن شبکه

232
00:17:39,029 --> 00:17:42,471
ReLU => Rectified Linear Unit

233
00:17:42,509 --> 00:17:49,438
اره - ی تابعیه که بیشترین مقدار بین 0 و ورودی اش رو محاسبه میکنه

234
00:17:49,438 --> 00:17:57,438
همونطوری که توی این ویدئو توضیح دادی، این تابع ی جورایی از زیست شناسی الهام گرفته شده

235
00:17:57,428 --> 00:18:05,785
که عصب ها فعال بشن یا نشن - اگر از ی حدی بگذره میشه تابع همانی

236
00:18:05,785 --> 00:18:09,643
ولی اگر از اون حد کمتر باشه، اون عصب فعال نمیشه یعنی برابر با 0 میشه

237
00:18:09,643 --> 00:18:11,192
که ی جورایی ساده تر میکنه کارو

238
00:18:11,192 --> 00:18:15,806
تابع سیگموید به تمرین دادن شبکه کمکی نکرد یا خیلی کار رو سخت میکرد بعضی مواقع

239
00:18:15,806 --> 00:18:24,708
بخاطر این اومدن از تابع "ریلو" استفاده کردن و با وجود شبکه های خیلی عمیق عصبی هم خوب عمل کرد

240
00:18:24,708 --> 00:18:26,114
ممنون از شما خانم لیشا

241
00:18:26,114 --> 00:18:35,056
داره میگه این شرکتی که این خانمه توش کار میکنه، روی شرکت های هوش مصنوعی سرمایه گذاری میکنه

242
00:18:35,056 --> 00:18:51,421
تبلیغشو میکنه

243
00:18:52,038 --> 00:19:12,538
ترجمه شدن توسط انجمن علمی مهندسی کامپیوتر دانشگاه شاهد - بهمن 1400
