1
00:00:04,259 --> 00:00:06,360
در اینجا ما به «پس انتشار» (انتشار معکوس) می پردازیم

2
00:00:06,360 --> 00:00:09,160
الگوریتم اصلی پشت پرده یادگیری شبکه های عصبی

3
00:00:09,160 --> 00:00:11,160
بعد از یک جمع بندی سریع تا جایی که هستیم

4
00:00:11,160 --> 00:00:15,048
اول یک راهنمای شهودی برای کاری که الگوریتم واقعا انجام میده

5
00:00:15,050 --> 00:00:17,562
بدون هیچ ارجاعی به فرمول ها بیان میکنیم

6
00:00:17,562 --> 00:00:20,062
بعد از این، برای افرادی که میخوان به ریاضیات بپردازن

7
00:00:20,064 --> 00:00:23,064
در ویدیوی بعدی به محاسبات زیر بنای همه این ها پرداخته میشه

8
00:00:23,765 --> 00:00:27,565
اگر دو ویدیوی آخر رو دیدید یا با پیش زمینه مناسب به این موضوع پرداختید

9
00:00:27,565 --> 00:00:31,065
میدونید که شبکه عصبی چی هست و چجوری اطلاعات رو (به لایه آخر) میرسونه

10
00:00:31,367 --> 00:00:34,865
اینجا ما یک مثال قدیمی از تشخیص ارقام دست نویس رو بررسی میکنیم

11
00:00:34,868 --> 00:00:39,368
که مقادیر پیکسل های اون به اولین لایه شبکه با 784 عصب وارد میشه

12
00:00:39,368 --> 00:00:43,868
و من شبکه ای با دو لایه مخفی رو نشون میدم که هر کدوم فقط 16 عصب دارن

13
00:00:43,868 --> 00:00:45,368
و یک لایه خروجی که از 10 عصب تشکیل شده

14
00:00:45,368 --> 00:00:49,570
و نشون میده که شبکه کدوم رقم رو به عنوان پاسخ خودش انتخاب میکنه

15
00:00:49,570 --> 00:00:51,070
همچنین من از شما انتظار دارم که

16
00:00:51,070 --> 00:00:54,271
گرادیان کاهشی رو همونطور که تو ویدیوی آخر (قسمت دوم) توضیح داده شد رو بلد باشید

17
00:00:54,271 --> 00:01:01,572
و اینکه منظور ما از یادگیری این هست که بفهمیم کدوم وزن ها تابع هزینه خاصی رو به کمترین میقدار میرسونن

18
00:01:01,572 --> 00:01:05,072
به عنوان یادآوری سریع، برای [محاسبه] هزینه یک مثال تمرینی

19
00:01:05,072 --> 00:01:06,572
کاری که باید انجام بدین اینه که

20
00:01:06,572 --> 00:01:11,072
خروجی ای که شبکه داده و خروجی ای که انتظار دارید ارائه بده رو دریافت کنید

21
00:01:11,072 --> 00:01:15,072
و فقط مربع های فاصله بین هر جزء رو با هم جمع کنید

22
00:01:15,075 --> 00:01:20,174
با انجام این کار برای ده ها هزار مثال تمرینی دیگه و میانگین گرفتن از نتایج

23
00:01:20,176 --> 00:01:22,676
به شما مقدار "هزینه" کل شبکه رو میده

24
00:01:22,677 --> 00:01:24,876
اما این برای بررسی کردن این موضوع کافی نیست

25
00:01:24,878 --> 00:01:27,677
همونطور که تو ویدیوی آخر توضیح داده شد، چیزی که ما دنبال اون هستیم

26
00:01:27,679 --> 00:01:30,679
گرادیان منفی این تابع هزینه هست

27
00:01:30,680 --> 00:01:35,578
که به شما میگه چجوری باید همه وزن ها و سوگیری ها یا اتصال های بین اونها رو تغییر بدین

28
00:01:35,581 --> 00:01:38,381
تا به بهترین نحو هزینه ها رو کاهش بدن

29
00:01:42,480 --> 00:01:49,182
الگوریتم "پس انتشار" که موضوع این ویدئو هم هست، الگوریتمی برای محاسبه اون گرادیان پیچیده هست

30
00:01:49,183 --> 00:01:54,483
و یک ایده از آخرین ویدیو(قسمت دوم) که من ازتون میخوام الان به خوبی در ذهنتون نگه دارید این هست که

31
00:01:54,483 --> 00:02:01,784
چون تصور بردار گرادیان به عنوان یک جهت در ابعاد 13000 ،به بیان ساده، فراتر از تصورات ماست

32
00:02:01,784 --> 00:02:03,784
روش دیگه ای هست که میتونید به اون فکر کنید

33
00:02:03,786 --> 00:02:11,286
بزرگی (مقدار) هر جزء در اینجا، به شما میگه که تابع هزینه، به هر وزن و سوگیری چقدر حساس هست

34
00:02:11,287 --> 00:02:14,787
برای مثال، فرض کنید شما فرآیندی رو که من میخوام توضیح بدم، طی کنید

35
00:02:14,788 --> 00:02:16,288
و گرادیان منفی رو محاسبه کنید

36
00:02:16,288 --> 00:02:21,288
و مولفه (جزء) مرتبط با وزن در این اتصال 3.2 بدست بیاد

37
00:02:21,288 --> 00:02:26,590
درحالی که مولفه مرتبط با این اتصال، در اینجا 0.1 باشه

38
00:02:26,591 --> 00:02:28,691
و چیزی که شما برداشت میکنید اینه که

39
00:02:28,692 --> 00:02:33,192
هزینه تابع 32 برابر بیشتر نسبت به تغییرات وزن اول حساس هست

40
00:02:33,193 --> 00:02:36,193
پس اگر بخواید اون مقدار رو فقط یک ذره تغییر بدید

41
00:02:36,193 --> 00:02:38,491
باعث ایجاد مقداری در تغییر در هزینه میشه

42
00:02:38,495 --> 00:02:43,495
و این تغییر 32 برابر بیشتر از چیزیه که وزن دوم ،با همون میزان تغییر (در مقادیر اولیه)، ایجاد میکنه

43
00:02:48,193 --> 00:02:50,995
شخصا زمانی که درباره «پس انتشار» (انتشار معکوس) یاد می گرفتم

44
00:02:50,996 --> 00:02:55,996
فکر میکنم گیج کننده ترین جنبه فقط نمادگذاری و اندیس گذاری همه اون علامت ها بود

45
00:02:55,997 --> 00:02:59,294
اما وقتی مشخص کنید هر بخش از این الگوریتم واقعا چه کاری رو انجام میده

46
00:02:59,298 --> 00:03:03,098
هر اثر منحصر به فردی که داره در واقع شهودی میشه

47
00:03:03,098 --> 00:03:07,098
و این فقط تعداد زیادی تنظیمات کوچکی هست که روی هم قرار میگیرن

48
00:03:07,098 --> 00:03:11,400
بنابراین من اینجا میخوام همه این کارها رو کاملا بدون توجه به نمادگذاری ها شروع کنم

49
00:03:11,401 --> 00:03:16,401
و فقط از طریق تاثیراتی که هر نمونه، روی وزن ها و سوگیری ها داره پیش برم

50
00:03:16,699 --> 00:03:23,602
از اونجایی که تابع هزینه شامل میانگین گیری از هزینه "هر" مثال، از همه ده ها هزار مثال تمرینی هست

51
00:03:23,603 --> 00:03:31,002
روشی که ما وزن ها و سوگیری ها رو برای هر مرحله از گرادیان کاهشی تنظیم میکنیم هم به هر مثال بستگی داره

52
00:03:31,198 --> 00:03:34,615
یا در اصل باید این کار انجام بشه ولی برای بهینه کردن محاسبات

53
00:03:34,616 --> 00:03:36,169
ما بعدا ترفند کوچکی انجام میدیم

54
00:03:36,169 --> 00:03:39,669
تا شما رو از محاسبه همه نمونه ها برای هر مرحله نجات بده

55
00:03:39,673 --> 00:03:40,872
درحال حاضر

56
00:03:40,872 --> 00:03:44,674
تنها کاری که میخوایم انجام بدیم این هست که توجه خودمون رو روی یک مثال متمرکز کنیم

57
00:03:44,675 --> 00:03:46,473
این یک تصویر از عدد 2

58
00:03:46,473 --> 00:03:51,973
این مثال تمرینی چه تاثیری باید روی نحوه تنظیم وزن ها و سوگیری ها داشته باشه؟

59
00:03:51,977 --> 00:03:54,977
فرض کنید در شرایطی هستیم که شبکه هنوز به خوبی تمرین داده نشده

60
00:03:54,978 --> 00:03:57,978
بنابراین "فعالسازی ها" در خروجی خیلی تصادفی به نظر میرسن

61
00:03:57,979 --> 00:04:02,479
شاید مقادیری مانند 0.5 ، 0.8 ، 0.2 و مانند اینها

62
00:04:02,479 --> 00:04:04,979
پس حالا نمیتونیم مستقیما اون "فعالسازی ها" رو تغییر بدیم

63
00:04:04,979 --> 00:04:07,479
ما فقط روی وزن ها و سوگیری ها تاثیر داریم

64
00:04:07,479 --> 00:04:12,979
اما توجه به تنظیماتی که میخوایم در لایه خروجی صورت بگیره، مفیده

65
00:04:12,983 --> 00:04:15,483
و از اونجایی که میخوایم تصویر رو به عنوان عدد "2" شناسایی کنیم

66
00:04:15,484 --> 00:04:21,483
میخوایم که اون مقدار سوم افزایش پیدا کنه؛ در حالی که بقیه مقادیر کاهش پیدا میکنن

67
00:04:21,485 --> 00:04:22,485
علاوه بر این

68
00:04:22,485 --> 00:04:25,086
اندازه این تغییرات (کاهش یا افزایش) باید

69
00:04:25,086 --> 00:04:29,887
با میزان فاصله هر مقدار فعلی از مقدار هدف خودش متناسب باشه

70
00:04:29,887 --> 00:04:30,887
به عنوان مثال

71
00:04:30,889 --> 00:04:35,088
افزایش مقدار فعالسازی عصب های شماره "2" ا

72
00:04:35,088 --> 00:04:38,588
مهمتر از کاهش مقدار فعالسازی عصب های شماره "8" هست

73
00:04:38,588 --> 00:04:40,588
که در حال حاضر خیلی نزدیک تر به جایی که هست که باید باشه

74
00:04:41,490 --> 00:04:45,091
بنابراین بیاید با بزرگنمایی بیشتر فقط روی این یک عصب تمرکز کنیم

75
00:04:45,091 --> 00:04:47,591
عصبی که میخوایم فعالسازی اون رو افزایش بدیم

76
00:04:47,591 --> 00:04:48,591
یادتون باشه که

77
00:04:48,593 --> 00:04:55,992
ا «فعالسازی» به عنوان مجموع وزنی مشخصی از همه فعالسازی های لایه قبلی به اضافه یک سوگیری تعریف میشه

78
00:04:55,994 --> 00:05:01,194
که بعد حاصلش به تابعی مثل ریلو یا سیگموید داده میشه
