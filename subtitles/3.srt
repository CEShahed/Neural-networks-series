1
00:00:04,259 --> 00:00:06,360
در اینجا ما به «پس انتشار» (انتشار معکوس) می پردازیم

2
00:00:06,360 --> 00:00:09,160
الگوریتم اصلی پشت پرده یادگیری شبکه های عصبی

3
00:00:09,160 --> 00:00:11,160
بعد از یک جمع بندی سریع تا جایی که هستیم

4
00:00:11,160 --> 00:00:15,048
اول یک راهنمای شهودی برای کاری که الگوریتم واقعا انجام میده

5
00:00:15,050 --> 00:00:17,562
بدون هیچ ارجاعی به فرمول ها بیان میکنیم

6
00:00:17,562 --> 00:00:20,062
بعد از این، برای افرادی که میخوان به ریاضیات بپردازن

7
00:00:20,064 --> 00:00:23,064
در ویدیوی بعدی به محاسبات زیر بنای همه این ها پرداخته میشه

8
00:00:23,765 --> 00:00:27,565
اگر دو ویدیوی آخر رو دیدید یا با پیش زمینه مناسب به این موضوع پرداختید

9
00:00:27,565 --> 00:00:31,065
میدونید که شبکه عصبی چی هست و چجوری اطلاعات رو (به لایه آخر) میرسونه

10
00:00:31,367 --> 00:00:34,865
اینجا ما یک مثال قدیمی از تشخیص ارقام دست نویس رو بررسی میکنیم

11
00:00:34,868 --> 00:00:39,368
که مقادیر پیکسل های اون به اولین لایه شبکه با 784 عصب وارد میشه

12
00:00:39,368 --> 00:00:43,868
و من شبکه ای با دو لایه مخفی رو نشون دادم که هر کدوم فقط 16 عصب دارن

13
00:00:43,868 --> 00:00:45,368
و یک لایه خروجی که از 10 عصب تشکیل شده

14
00:00:45,368 --> 00:00:49,570
و نشون میده که شبکه کدوم رقم رو به عنوان پاسخ خودش انتخاب میکنه

15
00:00:49,570 --> 00:00:51,070
همچنین من از شما انتظار دارم که

16
00:00:51,070 --> 00:00:54,271
گرادیان کاهشی رو همونطور که تو ویدیوی آخر (قسمت دوم) توضیح داده شد رو بلد باشید

17
00:00:54,271 --> 00:01:01,572
و اینکه منظور ما از یادگیری این هست که بفهمیم کدوم وزن ها تابع هزینه خاصی رو به کمترین مقدار میرسونن

18
00:01:01,572 --> 00:01:05,072
به عنوان یادآوری سریع، برای [محاسبه] هزینه یک مثال تمرینی

19
00:01:05,072 --> 00:01:06,572
کاری که باید انجام بدین اینه که

20
00:01:06,572 --> 00:01:11,072
خروجی ای که شبکه داده و خروجی ای که انتظار دارید ارائه بده، رو دریافت کنید

21
00:01:11,072 --> 00:01:15,072
و فقط مربع های فاصله بین هر جزء رو با هم جمع کنید

22
00:01:15,075 --> 00:01:20,174
با انجام این کار برای ده ها هزار مثال تمرینی دیگه و میانگین گرفتن از نتایج

23
00:01:20,176 --> 00:01:22,676
به شما مقدار "هزینه" کل شبکه رو میده

24
00:01:22,677 --> 00:01:24,876
اما این برای بررسی کردن این موضوع کافی نیست

25
00:01:24,878 --> 00:01:27,677
همونطور که تو ویدیوی آخر توضیح داده شد، چیزی که ما دنبال اون هستیم

26
00:01:27,679 --> 00:01:30,679
گرادیان منفی این تابع هزینه هست

27
00:01:30,680 --> 00:01:35,578
که به شما میگه چجوری باید همه وزن ها و انحراف معیار ها یا اتصال های بین اونها رو تغییر بدین

28
00:01:35,581 --> 00:01:38,381
تا به بهترین نحو هزینه ها رو کاهش بدن

29
00:01:42,480 --> 00:01:49,182
الگوریتم "پس انتشار" که موضوع این ویدئو هم هست، الگوریتمی برای محاسبه اون گرادیان پیچیده هست

30
00:01:49,183 --> 00:01:54,483
و یک چیز از آخرین ویدیو(قسمت دوم) که من ازتون میخوام الان خوب یادتون بمونه، اینه که

31
00:01:54,483 --> 00:02:01,784
چون تصور بردار گرادیان به عنوان یک جهت در ابعاد 13000 ،به بیان ساده، فراتر از تصورات ماست

32
00:02:01,784 --> 00:02:03,784
روش دیگه ای هست که میتونید به اون فکر کنید

33
00:02:03,786 --> 00:02:11,286
بزرگی (مقدار) هر جزء در اینجا، به شما میگه که تابع هزینه، به هر وزن و انحراف معیار چقدر حساس هست

34
00:02:11,287 --> 00:02:14,787
برای مثال، فرض کنید شما فرآیندی رو که من جلوتر توضیح بدم، رو انجام بدید

35
00:02:14,788 --> 00:02:16,288
و گرادیان منفی رو محاسبه کنید

36
00:02:16,288 --> 00:02:21,288
و مولفه (جزء) مرتبط با وزن در این اتصال 3.2 بدست بیاد

37
00:02:21,288 --> 00:02:26,590
درحالی که مولفه مرتبط با این اتصال، در اینجا 0.1 باشه

38
00:02:26,591 --> 00:02:28,691
و چیزی که شما برداشت میکنید اینه که

39
00:02:28,692 --> 00:02:33,192
هزینه تابع 32 برابر بیشتر نسبت به تغییرات وزن اول حساس هست

40
00:02:33,193 --> 00:02:36,193
پس اگر بخواید اون مقدار رو فقط یک ذره تغییر بدید

41
00:02:36,193 --> 00:02:38,491
باعث ایجاد مقداری در تغییر در هزینه میشه

42
00:02:38,495 --> 00:02:43,495
و این تغییر 32 برابر بیشتر از چیزیه که وزن دوم ،با همون میزان تغییر (در مقادیر اولیه)، ایجاد میکنه

43
00:02:48,193 --> 00:02:50,995
شخصا زمانی که درباره «پس انتشار» (انتشار معکوس) یاد می گرفتم

44
00:02:50,996 --> 00:02:55,996
فکر میکنم گیج کننده ترین جنبه فقط نمادگذاری و اندیس گذاری همه اون علامت ها بود

45
00:02:55,997 --> 00:02:59,294
اما وقتی مشخص کنید هر بخش از این الگوریتم واقعا چه کاری رو انجام میده

46
00:02:59,298 --> 00:03:03,098
هر اثر منحصر به فردی که داره در واقع شهودی میشه (یعنی میفهمید چی به چیه)ا

47
00:03:03,098 --> 00:03:07,098
و این فقط تعداد زیادی تنظیمات کوچکی هست که روی هم قرار میگیرن

48
00:03:07,098 --> 00:03:11,400
بنابراین من اینجا میخوام همه این کارها رو کاملا بدون توجه به نمادگذاری ها شروع کنم

49
00:03:11,401 --> 00:03:16,401
و فقط از طریق تاثیراتی که هر نمونه، روی وزن ها و انحراف معیار ها داره پیش برم

50
00:03:16,699 --> 00:03:23,602
از اونجایی که تابع هزینه شامل میانگین گیری از هزینه "هر" مثال، از همه ده ها هزار مثال تمرینی هست

51
00:03:23,603 --> 00:03:31,002
روشی که ما وزن ها و انحراف معیار ها رو برای هر مرحله از گرادیان کاهشی تنظیم میکنیم هم به هر مثال بستگی داره

52
00:03:31,198 --> 00:03:34,615
یا در اصل باید این کار انجام بشه ولی برای بهینه کردن محاسبات

53
00:03:34,616 --> 00:03:36,169
ما بعدا ترفند کوچکی انجام میدیم

54
00:03:36,169 --> 00:03:39,669
تا شما رو از محاسبه همه نمونه ها برای هر مرحله نجات بده

55
00:03:39,673 --> 00:03:40,872
درحال حاضر

56
00:03:40,872 --> 00:03:44,674
تنها کاری که میخوایم انجام بدیم این هست که توجه خودمون رو روی یک مثال متمرکز کنیم

57
00:03:44,675 --> 00:03:46,473
این یک تصویر از عدد 2

58
00:03:46,473 --> 00:03:51,973
این مثال تمرینی چه تاثیری باید روی نحوه تنظیم وزن ها و انحراف معیار ها داشته باشه؟

59
00:03:51,977 --> 00:03:54,977
فرض کنید در شرایطی هستیم که شبکه هنوز به خوبی تمرین داده نشده

60
00:03:54,978 --> 00:03:57,978
بنابراین "فعالسازی ها" در خروجی خیلی تصادفی به نظر میرسن

61
00:03:57,979 --> 00:04:02,479
شاید مقادیری مانند 0.5 ، 0.8 ، 0.2 و مانند اینها

62
00:04:02,479 --> 00:04:04,979
پس حالا نمیتونیم مستقیما اون "فعالسازی ها" رو تغییر بدیم

63
00:04:04,979 --> 00:04:07,479
ما فقط روی وزن ها و انحراف معیار ها کنترل داریم

64
00:04:07,479 --> 00:04:12,979
اما توجه به تنظیماتی که میخوایم در لایه خروجی صورت بگیره، مفیده

65
00:04:12,983 --> 00:04:15,483
و از اونجایی که میخوایم تصویر رو به عنوان عدد "2" شناسایی کنیم

66
00:04:15,484 --> 00:04:21,483
میخوایم که اون مقدار سوم افزایش پیدا کنه؛ در حالی که بقیه مقادیر کاهش پیدا کنن

67
00:04:21,485 --> 00:04:22,485
علاوه بر این

68
00:04:22,485 --> 00:04:25,086
اندازه این تغییرات (کاهش یا افزایش) باید

69
00:04:25,086 --> 00:04:29,887
با میزان فاصله هر مقدار فعلی از مقدار هدف خودش متناسب باشه

70
00:04:29,887 --> 00:04:30,887
به عنوان مثال

71
00:04:30,889 --> 00:04:35,088
افزایش مقدار فعالسازی عصب های شماره "2" ا

72
00:04:35,088 --> 00:04:38,588
مهمتر از کاهش مقدار فعالسازی عصب های شماره "8" هست

73
00:04:38,588 --> 00:04:40,588
که در حال حاضر خیلی نزدیک تر به جایی که هست که باید باشه

74
00:04:41,490 --> 00:04:45,091
بنابراین بیاید با بزرگنمایی بیشتر فقط روی این یک عصب تمرکز کنیم

75
00:04:45,091 --> 00:04:47,591
عصبی که میخوایم فعالسازی اون رو افزایش بدیم

76
00:04:47,591 --> 00:04:48,591
یادتون باشه که

77
00:04:48,593 --> 00:04:55,992
ا «فعالسازی» به عنوان حمع وزن داری مشخصی از همه فعالسازی های لایه قبلی به اضافه یک انحراف معیار تعریف میشه

78
00:04:55,994 --> 00:05:01,194
که بعد حاصلش به تابعی مثل ریلو یا سیگموید داده میشه


79
00:05:01,963 --> 00:05:07,463
پس 3 چیز مختلف وجود داره که دست به دست هم میدن تا مقدار فعالسازی رو افزایش بدن

80
00:05:07,665 --> 00:05:10,641
میتونی مقدار انحراف معیار رو زیاد کنی- میتونی مقدار وزن هارو زیاد کنی

81
00:05:10,641 --> 00:05:14,641
و میتونی مقدار فعالسازی عصب های لایه قبل رو افزایش بدی

82
00:05:14,773 --> 00:05:17,512
اگر فقط بخوایم روی افزایش وزن ها تمرکز کنیم

83
00:05:17,629 --> 00:05:21,199
توجه کنید که وزن ها، تاثیر های متفاوتی دارن

84
00:05:21,199 --> 00:05:25,656
اتصال بین روشن ترین عصب ها از لایه قبل، بیشترین تاثیر رو دارن

85
00:05:25,656 --> 00:05:29,338
از اونجایی که این وزن ها، با مقدار های فعالسازی بزرگتری ضرب میشن

86
00:05:30,834 --> 00:05:40,838
پس اگر یکی از این وزن هارو افزایش بدی، تاثیر بیشتری روی تابع هزینه داره تا افزایش وزن اتصال های عصب های تیره تر

87
00:05:40,862 --> 00:05:43,684
حداقل تا زمانی که این تمرین (شناسایی عدد 2) مد نظره

88
00:05:44,194 --> 00:05:46,704
یادتون باش وقتی در مورد "گرادیان کاهشی" صحبت میکنیم

89
00:05:46,704 --> 00:05:53,512
علاوه بر اینکه یک مقداری باید زیاد یا کم بشه توجه میکنیم، به این هم توجه میکنیم که کدوم تغییر باعث تاثیر بیشتری میشه

90
00:05:55,074 --> 00:06:01,675
این رفتار، ی جورایی شبیه یک نظریه در علم زیست شناسی در مورد یادگیری عصب ها است

91
00:06:01,831 --> 00:06:02,922
نظریه هبین

92
00:06:02,923 --> 00:06:06,923
که به طور خلاصه میگه، عصب هایی که باهم فعال میشن، به هم متصل هستن

93
00:06:07,084 --> 00:06:17,718
در اینجا، بیشترین افزایش در وزن ها،  بین عصب هایی که مقدار فعالسازی شون بیشتره و عصبی که میخوایم مقدار فعالسازی اش بیشتر بشه اتفاق میفته

94
00:06:17,718 --> 00:06:24,718
به بیان دیگه، اتصال بین عصب هایی که وقتی 2 رو میبینن فعال میشن، به عصب هایی که وقتی درباره 2 فکر میکنن فعال میشن، قوی تر میشه

95
00:06:24,937 --> 00:06:32,937
راستشو بخوای من توی جایگاهی نیستم که بگم شبکه های عصبی مصنوعی شبیه مغز کار میکنن یا نه

96
00:06:32,939 --> 00:06:37,454
و این ایده "اونایی که باهم فعال میشن، بهم متصل اند" از چند منابع علمی نتیجه گیری شده

97
00:06:37,456 --> 00:06:41,471
ولی به نظرم نکته خوبی واسه اشاره بود

98
00:06:42,822 --> 00:06:45,958
سومین روشی که میتونیم فعالسازی این عصب رو بیشتر کنیم

99
00:06:45,959 --> 00:06:48,973
اینه که همه مقدار های فعالسازی عصب های لایه قبلش رو زیاد کنیم

100
00:06:49,456 --> 00:06:57,915
به بیان دیگه، اگر همه اون عصب هایی که وزن مثبت دارن روشن تر بشن و عصب هایی که وزن منفی دارن تاریک تر بشن

101
00:06:57,918 --> 00:07:01,418
در نتیجه عدد فعالسازی عصب مورد نظرمون بیشتر میشه

102
00:07:02,646 --> 00:07:11,146
مثل روش تغییر وزن، بیشترین نتیجه رو با تغییر فعالسازی عصب ها با توجه به اندازه وزنشون میگیری

103
00:07:11,860 --> 00:07:15,098
خب قاعدتا نمیتونیم که فعالسازی این عصب های رو مستقیم تغییر بدیم

104
00:07:15,100 --> 00:07:18,105
ما فقط وزن ها و انحراف معیار ها رو میتونیم تغییر بدیم

105
00:07:18,105 --> 00:07:23,641
ولی از نظر لایه آخر، خوبه که بدونی چیا رو باید تغییر بدی

106
00:07:24,322 --> 00:07:29,995
ولی یادتون باشه، این تغییری بود که عصب مربوط به عدد 2 لازم داشت

107
00:07:29,995 --> 00:07:34,646
یادتون باشه که ما میخوایم عصب های دیگه توی این لایه فعالسازی شون کمتر بشه

108
00:07:34,937 --> 00:07:41,086
و هر کدوم از این عصب های لایه خروجی(آخر)، درباره اینکه توی لایه قبلش چه اتفاقاتی باید بیفته، نظر خودشونو دارن

109
00:07:42,879 --> 00:07:53,146
پس خواسته عصب مربوط به عدد 2 برای تغییر لایه قبلش، با خواسته همه عصب های عدد های دیگه جمع میشه

110
00:07:53,148 --> 00:08:01,148
این خواسته ها با توجه به مقدار وزن ها و مقدار نیاز به تغییر مقدار فعالسازی عصب مورد نظر، اعمال میشه

111
00:08:01,449 --> 00:08:05,872
اینجاست که ایده "انتشار به عقب" مطرح میشه

112
00:08:05,994 --> 00:08:13,494
با جمع کردن همه تغییر های خواسته شده، ی لیست از تغییرات برای لایه یکی به آخر بدست میارید

113
00:08:14,093 --> 00:08:21,093
وقتی این لیست رو بدست اوردی، میتونی این کار رو به صورت بازگشتی همین کارو برای این عصب (های لایه قبل) انجام بدی تا مقدارشون رو تنظیم کنی

114
00:08:21,199 --> 00:08:25,552
همین روند رو ادامه بدید و به صورت برعکس توی شبکه حرکت کنید

115
00:08:30,362 --> 00:08:37,235
یادتون باشه که این فقط یکی از این داده های تمرینی بود که میخواست این تغییرات رو روی همه وزن ها و انحراف معیار ها انجام بده

116
00:08:37,413 --> 00:08:43,793
اگر فقط تغییراتی که عدد 2 میخواد رو توی شبکه اعمال کنیم، شبکه فقط میتونه عکس هارو بر اساس 2 دسته بندی کنه ( یعنی بگه این تصویر 2 هست یا نیست )

117
00:08:43,947 --> 00:08:49,105
کاری که میکنیم اینه که همین روند "انتشار به عقب" رو برای همه داده های تمرینی دیگه هم انجام میدیم

118
00:08:49,105 --> 00:08:53,221
و بدست میاریم که هرکدوم چطوری میخوان وزن ها و انحراف معیار ها رو تغییر بدن

119
00:08:53,682 --> 00:08:56,649
و بعد این مقدار هارو میانگین میگیریم

120
00:09:02,081 --> 00:09:11,807
این مجموعه میانگین تغییر وزن ها و انحراف معیار ها، "تقریبا" برابر با منفی گرادیان کاهشی تابع هزینه هست، که قسمت قبل توضیح دادم

121
00:09:11,807 --> 00:09:13,951
یا حداقل ی نسبتی از اون

122
00:09:14,423 --> 00:09:19,460
بخاطر این که هنوز مقدار دقیق این تغییرات رو بدست نیودم، گفتم "تقریبا" ا

123
00:09:19,642 --> 00:09:26,836
ولی اگر تغییراتی که بهش اشاره کردم، و اینکه چرا نسبت تغییرات بعضیا از بقیه بیشتره، و چطوری باید باهم جمع بشن رو فهمیدی

124
00:09:26,836 --> 00:09:31,336
پس اینکه روش "پس انتشار" چطور کار میکنه رو فهمیدی

125
00:09:34,471 --> 00:09:42,743
در عمل خیلی طول میکشه که کامپیوتر که واسه هر مرحله از گرادیان کاهشی، بیاد همه تغییرات داده هارو باهم جمع کنه

126
00:09:42,744 --> 00:09:44,884
بجاش معمولا این کارو میکنن

127
00:09:44,885 --> 00:09:49,984
داده های تمرینی ات رو بهم میریزی و به کلی گروه کوچیک تقسیمشون میکنی

128
00:09:50,153 --> 00:09:52,700
مثلا هر گروه 100 تا داده تمرینی داشته باشه

129
00:09:53,230 --> 00:09:56,711
و بعد یک قدم (قدم گرادیان کاهشی) رو برای ی گروه کوچیک حساب میکنی

130
00:09:56,908 --> 00:10:02,749
قاعدتا مقدارش برابر با قدم گرادیان کاهشی واقعی نمیشه چون به کل داده های تمرینی بستگی داره نه فقط ی بخش کوچیکش

131
00:10:02,750 --> 00:10:05,988
پس یعنی بهترین قدم (قدم گرادیان کاهشی تابع هزینه) نیست، ولی

132
00:10:05,989 --> 00:10:12,166
ولی هر گروه کوچیک تقریب خوبی بهت میده و از همه مهم تر، سرعت انجام این کار رو به طور قابل توجهی بالا میبره

133
00:10:12,822 --> 00:10:16,610
اگر مسیر حرکت رو روی "رویه" تابع هزینه مشخص کنی

134
00:10:16,817 --> 00:10:21,763
مثل این میمونه ی آدم مست بدون هدف میاد به سمت گودال ولی "سریع" قدم برمیداره

135
00:10:21,764 --> 00:10:30,764
در مقابل ی آدم دقیق که قبل از هر قدم "کوچیک" و با دقت اش، کلی حساب وکتاب میکنه تا تصمیم بگیره کدوم جهت بره

136
00:10:31,263 --> 00:10:35,182
به این روش میگن  "گرادیان کاهشی تصادفی" ا

137
00:10:35,815 --> 00:10:39,918
خب ی جمع بندی از چیزایی که گفتیم داشته باشیم

138
00:10:40,186 --> 00:10:47,197
ا "پس انتشار" الگوریتمیه که مشخص میکنه هر یدونه داده تمرینی وزن ها و انحراف معیار هارو چقدر و چجوری میخواد تغییر بده

139
00:10:47,197 --> 00:10:55,697
نه فقط اینکه این مقادیر رو کم و زیاد کنه، بلکه اینکه چه نسبتی از این مقادیر باید تغییر کنه تا باعث بیشترین کاهش هزینه بشه

140
00:10:55,937 --> 00:11:04,408
ی قدم گرادیان کاهشی واقعی، اینطوری بدست میاد که این کارو واسه ده هزار ها داده تمرینی ات انجام بدی و درآخر ازشون میانگین بگیری

141
00:11:04,408 --> 00:11:06,437
ولی محاسبه به این روش خیلی کنده

142
00:11:06,437 --> 00:11:10,317
بجاش داده های تمرینی ات رو بهم میریزی و به گروه های کوچک دسته بندی میکنی

143
00:11:10,317 --> 00:11:13,793
و هر قدم (قدم گرادیان کاهشی) رو با توجه به یک گروه کوچیک محاسبه میکنی

144
00:11:13,870 --> 00:11:17,589
اگر این کارو واسه همه گروه ها تکرار کنی و تغییرات مربوطشون روی شبکه اعمال کنی

145
00:11:17,590 --> 00:11:21,149
به ی مینیمم نسبی از تابع هزینه میرسی

146
00:11:21,149 --> 00:11:26,975
که به این معنیه که در آخر شبکه ات عملکرد خوبی (در تشخیص) داده های تمرینی پیدا کرده

147
00:11:27,475 --> 00:11:32,125
با همه چیزایی که گفته شد، کد هایی که الگوریتم "پس انتشار" رو پیاده سازی کرده

148
00:11:32,125 --> 00:11:37,046
به چیزی اشاره میکنه که تا حالا دیدید - حداقل به روش غیررسمی بیان شده

149
00:11:37,431 --> 00:11:40,845
ولی بعضی موقع ها دونستن اینکه قسمت ریاضی اش چیکار میکنه، نصف کاره

150
00:11:40,971 --> 00:11:44,706
و نوشتن و طراحی اونا جاییه که کار پیچیده میشه

151
00:11:45,024 --> 00:11:47,437
پس برای کسایی که میخوان عمیق تر با این موضوع آشنا بشن

152
00:11:47,437 --> 00:11:52,437
قسمت بعدی سراغ همه چیزایی که این قسمت گفتیم میریم ولی براساس [فرمول های] ریاضی پشت پرده

153
00:11:52,440 --> 00:11:56,985
که باعث میشه باهاش بیشتر آشنا بشید

154
00:11:57,576 --> 00:12:03,859
به این نکته اشاره کنم که برای اینکه این الگوریتم و کلا همه الگوریتم های هوش مصنوعی کار کنه

155
00:12:03,860 --> 00:12:06,186
به کلی داده تمرینی نیاز دارید

156
00:12:06,187 --> 00:12:09,519
چیزی که باعث میشه مثال تشخیص اعداد نوشته شده با دست ی مثال خوبی باشه

157
00:12:09,522 --> 00:12:15,022
وجود دیتابیس ... هست که توش کلی داده داره که توسط آدم برچسب گذاری شده (اطلاعاتش استخراج شده) ا

158
00:12:15,158 --> 00:12:18,995
ی مشکل عمومی که برای اونایی که توی حوزه یادگیری ماشین کار میکنن باهاش آشنا هستن

159
00:12:18,998 --> 00:12:22,014
اینه که داده های برچسب گذاری شده ای که میخوای رو تهیه کنی

160
00:12:22,014 --> 00:12:27,649
که میتونه توسط انسان برچسب گذاری شده یا چیز دیگه ای

161
00:12:27,652 --> 00:12:33,081
این مشکل مارو هدایت میکنه اسپانسر ما "کورد فلور" ا

162
00:12:33,096 --> 00:12:38,115
بستر نرم افزاری هست که مهندسین داده و تیم های یادگیری ماشین میتونن داده تمرینی درست کنن

163
00:12:38,509 --> 00:12:44,009
بهت این اجازه رو میدن که اطلاعات و داده هات رو آپلود کنی و به صورت برچسب گذاری شده توسط آدمای واقعی دریافتش کنی

164
00:12:44,211 --> 00:12:49,173
شاید قبلا در مورد روش "انسان در چرخه کار" شنیده باشی، این همونه

165
00:12:49,427 --> 00:12:52,850
استفاده از هوش انسان برای تمرین هوش ماشین

166
00:12:53,606 --> 00:12:58,133
اونا از ساختار خاصی استفاده میکنن تا داده هارو دقیق و تمیز دربیارن

167
00:12:58,134 --> 00:13:02,205
و به هزاران پروژه هوش مصنوعی کمک کرده تا اونا تمرین بدن و تست کنن و بهبود بدن

168
00:13:02,716 --> 00:13:14,188
میگه اگر بری توی ی لینکه و ثبت نام کنی تیشرت مجانی واست میفرسته

169
00:13:14,190 --> 00:13:21,514
بدرد ما که نمیخوره کسی نمیتونه چیزی بفرسته ایران

170
00:13:21,514 --> 00:13:28,514
داره از اسپانسر و همه کسایی که بهش کمک مالی کردن تشکر میکنه

171
00:13:29,538 --> 00:13:53,038
**ترجمه شده توسط انجمن مهندسی کامپیوتر دانشگاه شاهد - بهمن 1400**