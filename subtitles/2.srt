1
00:00:03,964 --> 00:00:07,464
توی ویدئوی قبل، من ساختار یک شبکه عصبی رو بهتون نشون دادم

2
00:00:07,387 --> 00:00:13,006
اول ی خلاصه میگم تا ذهنمون آماده بشه و بعد دو تا هدف اصلی واسه این ویدئو دارم

3
00:00:13,006 --> 00:00:16,158
اولی اینه که بهتون فلسفه "گرادیان کاهشی" رو توضیح بدم

4
00:00:16,158 --> 00:00:20,696
این مفهوم علاوه بر اینکه مبنای یادگیری شبکه عصبی هست، مبنای یادگیری الگوریتم های دیگه ی یادگیری ماشین هم هست

5
00:00:20,696 --> 00:00:28,696
بعد یکم در مورد اینکه این شبکه عصبی که مثال زدم چطور کار میکنه و لایه های مخفی(وسط) دنبال چی هستن، توضیح میدم

6
00:00:28,710 --> 00:00:29,620
** خلاصه قسمت قبل**

7
00:00:29,620 --> 00:00:34,405
اگر یادتون باشه، هدف ما تشخیص اعداد نوشته شده با دست هست

8
00:00:34,420 --> 00:00:36,707
اولین کاری که هرکسی که شبکه عصبی میخواد یادبگیره انجام میده

9
00:00:36,777 --> 00:00:43,277
این عکس ها به صورت یک جدول 28 در 28 هستن که هر پیکسل یک رنگ سیاه و سفید بین 0 و 1 داره

10
00:00:43,447 --> 00:00:50,447
این پیکسل ها، مقدار "فعالسازی" کل 784 تا عصب لایه ورودی(لایه اول) شبکه عصبی مون هستن

11
00:00:50,996 --> 00:01:01,633
و مقدار "فعالسازی" برای عصب های لایه بعد، براساس جمع وزن دار مقدار های فعالسازی لایه قبل به علاوه ی عدد خاص که بهش میگن عدد انحراف معیار، هست

12
00:01:01,896 --> 00:01:09,396
و بعد حاصل رو با ی تابع مثل سیگموید یا ریلو ترکیب میکنی - همونطور که توی قسمت قبل گفتم

13
00:01:09,468 --> 00:01:14,581
با انتخاب دوتا لایه پنهان که هرکدوم 16 تا عصب دارن ( که تعدادش رو خودم همینطوری انتخاب کردم)

14
00:01:14,581 --> 00:01:19,134
کل شبکه سیزده هزار تا وزن و عدد انحراف معیار داره که میتونیم مقدارشون رو تغییر بدیم

15
00:01:19,134 --> 00:01:24,634
و این مقدار ها هستن که مشخص میکنن که این شبکه اصلی دقیقا چیکار میکنه

16
00:01:24,781 --> 00:01:28,584
وقتی میگم این شبکه عصبی عددهایی که بهش داده میشه رو دسته بندی میکنه

17
00:01:28,584 --> 00:01:33,352
یعنی روشن ترین (فعال ترین) عصب از 10 عصب لایه آخر، اون عدد رو نشون میده

18
00:01:33,686 --> 00:01:41,186
یادتون باشه انگیزه چند لایه کردن این شبکه عصبی این بود که "احتمالا" لایه دوم بتونه گوشه هارو پیدا کنه

19
00:01:41,513 --> 00:01:45,063
و لایه سوم بتونه الگوهایی مثل خط و حلقه رو پیدا کنه

20
00:01:45,063 --> 00:01:49,671
و لایه آخر بتونه از الگو های تشخیص داده شده توی لایه قبل رو نتیجه گیری کنه و عدد مربوطه رو تشخیص بده

21
00:01:49,671 --> 00:01:52,507
خب توی این قسمت میفهمیم که شبکه عصبی چطور یادمیگیره

22
00:01:52,643 --> 00:02:03,846
چیزی که میخوایم، یک الگوریتم هست که بتونی بهش کلی داده تمرینی نشون بدی که به صورت عکس به همراه عددی که توشون نوشته شده هستن

23
00:02:03,846 --> 00:02:10,347
و اون الگوریتم با استفاده از اون داده ها، اون 13 هزار متغیر اش طوری تغییر بده که بتونه بهتر حدس بزنه

24
00:02:10,407 --> 00:02:16,997
خوشبختانه با استفاده از ساختار چند لایه ای، چیزی که این شبکه یادمیگیره رو به فراتر از داده هایی که به عنوان تمرین بهش داده شده هم تعمیم میده

25
00:02:17,145 --> 00:02:23,407
و وقتی که شبکه رو با ی سری داده تمرین دادی، میتونی با نشون دادن داده های جدید که تا حالا ندیده، این رو آزمایش کنی

26
00:02:23,407 --> 00:02:26,966
و متوجه میشی که این شبکه با چ دقتی تصاویر رو دسته بندی میکنه

27
00:02:30,616 --> 00:02:34,336
چیزی که باعث میشه که "تشخیص عدد" یک کار خیلی رایج برای شروع کار با شبکه عصبی باشه

28
00:02:34,336 --> 00:02:44,336
اینه که آدمای خوبی که پشت دیتابیس ... هستن، ده ها هزار عکس عدد نوشته شده به همراه مقدار واقعی شون رو جمع آوری کردن

29
00:02:44,663 --> 00:02:48,251
این که توضیح بدی کامپیوتر چطور یادمیگیره، خیلی حیرت برانگیزه

30
00:02:48,251 --> 00:02:55,872
وقتی که فهمیدی چطوری کار میکنه، بیشتر شبیه ی تمرین حسابان میشه تا وعده های علمی تخیلی

31
00:02:55,872 --> 00:03:00,241
ی جورایی به توی پیدا کردن کمترین مقدار یک تابع خلاصه میشه

32
00:03:02,122 --> 00:03:07,979
این رو در نظر داشته باشید که فرض میکنیم که هر عصب به همه عصب های لایه قبلش متصله

33
00:03:08,145 --> 00:03:14,407
وزن ها توی معادله جمع وزن دار، ی جورایی مشخص کننده شدت اون اتصال هستن

34
00:03:14,407 --> 00:03:19,461
و "انحراف معیار" عددیه که مشخص میکنه عصب باید فعال بشه یا نه

35
00:03:19,461 --> 00:03:24,741
برای شروع، همه وزن ها و انحراف معیار هارو ی عدد شانسی انتخاب میکنیم

36
00:03:24,818 --> 00:03:30,818
از اونجایی که همه متغیر هارو شانسی انتخاب کردیم، واضحه که نتیجه خیلی وحشتناک باشه

37
00:03:31,038 --> 00:03:36,133
برای مثال به عنوان ورودی، عکس عدد 3 رو میدی و لایه خروجی، اوضاعش خیلی خیطه

38
00:03:36,257 --> 00:03:39,288
کاری که میکنیم اینه که ی تابع به نام "تابع هزینه" تعریف میکنیم

39
00:03:39,288 --> 00:03:48,973
این تابع روشیه که به کامپیوتر بگیم: ای کامپیوتر بد! خروجی این عکس باید عصب های لایه آخر غیر فعال باشن و یکیشون فعال بشه

40
00:03:48,973 --> 00:03:51,453
ولی تو به عنوان نتیجه بهم ی چیز چرت دادی

41
00:03:51,453 --> 00:04:01,453
نحوه بیان ریاضی اش میشه: که مربع تفاضل بین خروجی دلخواه و خروجی چرتی که کامپیوتر تحویل داده رو حساب میکنیم

42
00:04:01,524 --> 00:04:05,233
به این میگیم "هزینه" یک داده تمرینی.

43
00:04:05,233 --> 00:04:11,014
توجه کنید که مقدار این جمع وقتی که عکس ورودی رو به درستی تشخیص میده کمه

44
00:04:12,014 --> 00:04:16,346
ولی اگر اشتباه تشخیص بده مقدارش زیاده

45
00:04:18,372 --> 00:04:26,372
کاری میکنیم اینه که مقدار میانگین تابع هزینه رو به ازای همه ده ها هزار ورودی که بهش دادی حساب میکنی

46
00:04:26,675 --> 00:04:32,997
این مقدار هزینه میانگین، سنجشی هست که بفهمیم چقدر بد عمل میکنه

47
00:04:32,997 --> 00:04:34,992
و این کار خیلی پیچیده ایه!

48
00:04:34,992 --> 00:04:37,975
یادته گفتم کل شبکه مثل ی تابع میمونه؟

49
00:04:37,975 --> 00:04:45,163
تابعی که 784 تا عدد به عنوان ورودی میگیره (مقدار پیکسل ها) و 10 تا عدد خروجی میده

50
00:04:45,163 --> 00:04:49,425
و نحوه کار این تابع با 13 هزار متغیر تعیین میشه

51
00:04:49,425 --> 00:04:52,858
تابع هزینه، یک لایه پیچیدگی بر مبنای این محسوب میشه

52
00:04:52,858 --> 00:04:57,358
این تابع به عنوان ورودی اون 13 هزار تا وزن و انحراف معیار رو میگیره

53
00:04:57,346 --> 00:05:02,346
و خروجی ی عدد میده که مشخص کننده اینه که وضعیت اون وزن ها و انحراف معیار ها چقدر بده

54
00:05:02,538 --> 00:05:09,038
و این تابع، به رفتار شبکه نسبت به همه ده هزار داده ورودی مختلف تمرینی بستگی داره

55
00:05:09,353 --> 00:05:11,353
اگر بهش فکر کنی، متوجه میشی که خیلی زیاده

56
00:05:11,795 --> 00:05:15,954
ولی این که فقط به کامپیوتر بگی چقد بد عمل میکنه، کافی نیست

57
00:05:15,887 --> 00:05:20,857
باید بهش بگی که چطور وزن ها و انحراف معیار هاش رو عوض کنه تا بهتر بشه

58
00:05:21,076 --> 00:05:30,576
برای ساده سازی، بجای این که تابعی با 13 هزار ورودی تصور کنی، ی تابع رو در نظر بگیر که ی ورودی داره و ی خورجی

59
00:05:31,348 --> 00:05:35,516
چطور ورودی ای پیدا میکنی(ایکس) رو پیدا میکنی که جوابش(خروجی تابع - ایگرگ) کمترین بشه؟

60
00:05:36,415 --> 00:05:40,781
اونایی که حسابان گذروندن، میدونن که بعضی موقع ها میشه کمترین مقدار رو سریع محاسبه کنی

61
00:05:40,781 --> 00:05:44,341
ولی این روش ها برای تابع های پیچیده، راحت و قابل استفاده نیستن

62
00:05:44,341 --> 00:05:51,228
حداقل برای تابع هزینه ما که 13 هزار تا ورودی داره

63
00:05:51,228 --> 00:05:59,442
ی راه حل عمومی تر اینه که از ی جایی شروع کنی و مشخص کنی که به کدوم سمت حرکت کنی تا مقدار تابع کمتر بشه

64
00:05:59,461 --> 00:06:10,461
یعنی اگر بتونی شیب تابع رو توی اون نقطه بدست بیاری، اگر شیب مثبت بود باید بری چپ، اگر منفی بود بری راست

65
00:06:12,461 --> 00:06:20,125
اگر چند بار این کار رو انجام بدی، به یکی از نقاط "مینیمم نسبی" تابع میرسی

66
00:06:20,125 --> 00:06:24,180
میتونی اینطور تصور کنی که ی توپ از بالای تپه سر میخوره به پایین

67
00:06:24,180 --> 00:06:33,495
حتی برای این تابع تک ورودی ساده هم چند تا مقدار مینیمم نسبی مختلف ممکنه بدست بیاری (با توجه به اینکه از نقطه اول رو کجا انتخاب کنی)

68
00:06:33,495 --> 00:06:40,004
ممکنه این مینیمم هایی نسبی که بدست میاری، هیچکدوم کمترین مقدار تابع هزینه نباشه

69
00:06:40,004 --> 00:06:42,651
این قضیه شامل شبکه عصبی هم میشه

70
00:06:42,896 --> 00:06:47,766
نکته ای که میخوام بدونید اینه که اگر قدم هایی که برمیدارید متناسب با "اندازه" شیب باشه

71
00:06:47,766 --> 00:06:52,766
برای مثال جایی که شیب نزدیک نقطه مینیمم نسبی کم میشه، قدم های شما هم کوچک تر میشه

72
00:06:52,762 --> 00:06:55,716
و این باعث میشه از نقطه مینیمم پرت نشید

73
00:06:55,970 --> 00:07:00,978
برای این که سطح پیچیدگیش رو که بیشتر کنیم، فرض کنید ی تابع داریم که دو تا ورودی میگیره و ی خروجی میده

74
00:07:00,978 --> 00:07:08,088
میتونی تابع رو به صورت ی صفحه در نظر بگیری و تابع هزینه اش رو یک "رویه" بالای اون صفحه

75
00:07:08,358 --> 00:07:11,120
توی این مثال بجای اینکه دنبال شیب نمودار بگردی

76
00:07:11,120 --> 00:07:19,120
باید دنبال جهتی بگردی که اگر به سمتش حرکت کنی، مقدار خروجی تابع سریعتر کم میشه

77
00:07:19,552 --> 00:07:26,129
یا به بیان دیگه (توی تصویر سمت راست) جهت گودال کجاست؟ میتونید مسئله رو به این صورت فرض کنید که توپی  از تپه به سمت پایین سر میخوره

78
00:07:26,639 --> 00:07:34,639
کسایی ریاضی2 پاس کردن، میدونن که گرادیان تابع توی اون نقطه، بهتون جهت شدیدترین سربالایی رو میده

79
00:07:34,843 --> 00:07:39,002
یعنی جهتی رو بهت میده که اگر به سمتش حرکت کنی، مقدار تابع سریعتر زیاد میشه

80
00:07:39,031 --> 00:07:46,293
اگر منفیش کنیم، بهمون جهتی رو میده که اگر در سمتش حرکت کنیم، مقدار تابع سریعتر کم میشه

81
00:07:46,966 --> 00:07:54,466
اندازه بردار گرادیان مشخص کننده اینه که اون سراشیبی، چقدر شیبش زیاده

82
00:07:54,610 --> 00:08:00,478
میگه اگر ریاضی2 بلد نیستی به دوره ای که توی سایت خان آکادمی گذاشتم ی نگاه بنداز

83
00:08:00,721 --> 00:08:08,141
درنهایت تنها چیزی که برای من و شما اهمیت داره، اینه که بدونیم بالاخره ی راه حلی برای پیدا کردن اون بردار وجود داره

84
00:08:08,141 --> 00:08:11,980
برداری که بهت میگه جهت گودال کجاست و شیبش چقدره

85
00:08:12,086 --> 00:08:16,293
این کافیه و لازم نیست از جزئیات محاسبه اش خیلی خبر داشته باشی

86
00:08:17,071 --> 00:08:20,764
اگر درست متوجه شده باشی، روش پیدا کردن کمترین نقطه های تابع، اینه که

87
00:08:20,764 --> 00:08:27,793
اینه که جهت گرادیان رو پیدا کنی و به سمت پایین حرکت کنی ( و این کارو انقد ادامه بدی که عمق کمتری وجود نداشته باشه )

88
00:08:27,978 --> 00:08:32,884
به صورت کلی، روش حلش برای تابعی که 13000 تا ورودی هم داره شبیه اینه

89
00:08:33,028 --> 00:08:39,697
فرض کنید که همه وزن ها و انحراف معیار هارو توی ی بردار بزرگ بزاری

90
00:08:39,999 --> 00:08:44,100
مقدار منفی تابع گرادیان هم ی برداره

91
00:08:44,100 --> 00:08:54,980
ی جهت توی اون فضای بزرگ که میگه کدوم سمت اگر بری باعث میشه مقدار تابع هزینه ات سریعتر کم بشه

92
00:08:55,413 --> 00:09:01,341
با توجه به تابع هزینه مخصوصی که ما طراحی کردیم، تغییر دادن وزن ها و انحراف معیار ها برای اینکه مقدار تابع هزینه کمش کنیم

93
00:09:01,341 --> 00:09:10,682
به این معنیه که خروجی نهایی شبکه برای هر داده رو ی کاری کنیم که بیشتر شبیه ی تصمیم گیری باشه تا ی انتخاب شانسی

94
00:09:11,519 --> 00:09:16,738
یادتون باشه که تابع هزینه شامل میانگین همه داده های تمرینی میشه

95
00:09:16,738 --> 00:09:21,865
اگر کمترین مقدارش رو بدست بیاری، یعنی اینکه توی همه داده های تمرینی بهترین عملکرد رو داره

96
00:09:24,129 --> 00:09:30,022
اسم الگوریتمی که برای محاسبه بهینه این گرادیان به کار میره، که قاعدتا قلب یادگیری شبکه عصبی هم هست

97
00:09:30,022 --> 00:09:31,740
الگوریتم "پس انتشار" هست

98
00:09:31,826 --> 00:09:34,311
که توی قسمت بعدی در موردش حرف میزنم

99
00:09:34,480 --> 00:09:41,947
الان میخوام توضیح بدم وقتی که داده تمرینی جدیدی داده میشه، برای هر وزن و انحراف معیار دقیقا چ اتفاقی میفته

100
00:09:41,947 --> 00:09:47,149
میخوام بهتون درک اینی که چ اتفاقی میفته بدم - ی چیزی فراتر کوهی از فرمول های ریاضی

101
00:09:47,500 --> 00:09:51,903
چیزی که الان میخوام بفهمید جدای از جزئیات

102
00:09:51,903 --> 00:09:58,903
اینکه وقتی میگیم "یادگیری شبکه"، یعنی کاری کنیم که تابع هزینه کمترین مقدار بشه

103
00:09:59,096 --> 00:10:04,480
این مهمه که تابع هزینه توی هر مرحله، ی خروجی کند (با تغییرات کم) داشته باشه

104
00:10:04,480 --> 00:10:08,610
اینطوری میتونیم با قدم های کوچیک به سمت پایین، مینیمم نسبی رو پیدا کنیم

105
00:10:08,822 --> 00:10:14,195
این دلیلیه که عصب های مصنوعی مقدارشون به صورت "پیوسته" تغییر میکنه

106
00:10:14,214 --> 00:10:19,714
نه اینکه 0 "یا" 1 باشن - درست شبیه چیزی که عصب های طبیعی رفتار میکنن

107
00:10:20,089 --> 00:10:27,115
به فرایند تغییر دادن ورودی تابع با گرادیان منفی رو بهش میگن "گرادیان کاهشی" ا

108
00:10:27,115 --> 00:10:30,903
ی روشیه برای این که به مقدار مینیمم نسبی تابع هزینه برسیم

109
00:10:30,903 --> 00:10:33,211
که معادلش توی تصویر میشه ی گودال

110
00:10:33,392 --> 00:10:40,865
من هنوز دارم تصویر ی تابع با دوتا ورودی رو بهتون نشون میدم، چون تصور کردن ی تابع توی فضای سیزده هزار بعدی سخته

111
00:10:41,028 --> 00:10:44,951
ولی ی روشی وجود داره که بشه بهش فکر کرد

112
00:10:45,134 --> 00:10:48,518
هر جزء گرادیان منفی دوتا چیز رو بهمون میگه

113
00:10:48,639 --> 00:10:55,403
علامت اون جزء بهمون میگه جزء متناظرش باید زیاد بشه یا کم بشه

114
00:10:55,576 --> 00:11:03,221
و اندازه هر جزء هم بهمون میگه که کدوم تغییر مهم تره

115
00:11:05,889 --> 00:11:13,721
همونطور که میبنی، تغییر دادن یکی از وزن ها نسبت به بقیه، میتونه تاثیر بیشتری روی تابع هزینه مون داشته باشه

116
00:11:14,648 --> 00:11:18,287
بعضی از این اتصال ها برای داده های تمرینی ما، اهمیت بیشتری دارن

117
00:11:19,200 --> 00:11:24,221
ی طور دیگه که میتونی به این بردار گردیانِ تابع پیچیده هزینه مون نگاه کنی

118
00:11:24,221 --> 00:11:28,552
اینه که میاد اطلاعات اهمیت تغییر هر وزن و انحراف معیار رو کد گذاری میکنه

119
00:11:28,552 --> 00:11:32,918
که کدوم از این تغییر ها ارزش و تاثیر بیشتری دارن

120
00:11:33,921 --> 00:11:37,000
این نتیجه گیری، فقط یک نحوه دیگه نگاه کردن به بردار جهت هست

121
00:11:37,000 --> 00:11:41,466
برای مثال، اگر ی تابع داشته باشی که دوتا عدد به عنوان ورودی بگیره

122
00:11:41,466 --> 00:11:47,466
و گرادیانش رو توی ی نقطه حساب کنی، و جوابش بشه 3 و 1

123
00:11:47,595 --> 00:11:55,782
میتونی اینطوری تفسیرش کنی که وقتی توی اون نقطه هستی، با حرکت به این جهت، مقدار تابع سریعتر زیاد میشه

124
00:11:55,908 --> 00:12:02,471
وقتی که تابع رو بالای صفحه رسم کنی، میبینی که بردار حاصل، بهت ی سربالایی مستقیم میده

125
00:12:02,471 --> 00:12:10,471
ولی ی نحوه دیگه تفسیرش هم اینه که تغییر توی متغیر اول، اولویت 3 برابری داره نسبت به تغییر متغیر دوم

126
00:12:10,471 --> 00:12:17,471
یعنی حداقل نزدیک اون ورودی، تغییر دادن مقدار ایکس، ارزش و اهمیت بیشتری داره

127
00:12:19,836 --> 00:12:22,619
خیل خب، بیاید ی جمع بندی بکنیم

128
00:12:22,812 --> 00:12:27,522
کل شبکه، ی تابع با 784 تا ورودی و 10 خروجی هست

129
00:12:27,522 --> 00:12:30,423
که با همه جمع های وزن دار اش توصیف میشه

130
00:12:30,649 --> 00:12:33,735
تابع هزینه، یک لایه پیچیدگی بر مبنای اون هست

131
00:12:33,788 --> 00:12:37,495
این تابع 13 هزار وزن و انحراف معیار رو به عنوان ورودی میگیره

132
00:12:37,495 --> 00:12:41,965
و بر اساس داده های تمرینی که بهش داده شده، ی مقدار به عنوان مشخص کننده بد بودن نتیجه، خروجی میده

133
00:12:42,373 --> 00:12:47,099
و محاسبه گرادیان تابع هزینه، خودش پیچیدگی های خودشو داره

134
00:12:47,099 --> 00:12:54,113
بهمون میگه که چ تغییری توی همه این وزن ها و انحراف معیار ها،  باعث سریعترین تغییر توی تابع هزینه میشه

135
00:12:54,139 --> 00:12:58,182
که به این معنیه که کدوم تغییر برای کدوم وزن ها و انحراف معیار ها اهمیت بیشتری داره

136
00:13:02,618 --> 00:13:06,278
وقتی که شبکه عصبی رو براساس مقدار های شانسی ایجاد میکنی

137
00:13:06,278 --> 00:13:09,639
و بارها براساس فرایند "گرادیان کاهشی" تنظیمش میکنی

138
00:13:09,719 --> 00:13:13,706
روی عکسایی که ندیده چقد خوب عمل میکنه؟

139
00:13:13,975 --> 00:13:20,975
شبکه عصبی که من نشونتون دادم که شامل دوتا لایه مخفی با هرکدوم شامل 16 تا عصب میشه

140
00:13:20,990 --> 00:13:26,224
نتیجه بد نیست - 96 درصد عکسای جدید رو تونسته درست دسته بندی کنه

141
00:13:26,774 --> 00:13:29,932
اگر به چند تا از مثال هایی که نتونسته تشخیص بده نگاه کنی

142
00:13:29,932 --> 00:13:33,932
بهش یکم حق میدی

143
00:13:35,820 --> 00:13:41,697
اگر یکم ساختار این لایه های مخفی یکم تغییر بدی، میتونی به دقت 98 درصد هم برسی

144
00:13:41,831 --> 00:13:43,883
عالی نیست ولی خیلی خوبه

145
00:13:43,883 --> 00:13:48,883
میتونی با پیچیده کردن این شبکه عصبی ساده، نتیجه بهتری بگیری

146
00:13:48,985 --> 00:13:51,634
ولی این رو هم در نظر بگیر که کاری که انجام میده چقد وحشتناکه

147
00:13:51,634 --> 00:13:57,134
به نظرم ی چیز شگفت انگیز در مورد همه شبکه هایی وجود داره که روی داده هایی که ندیده نتیجه خوبی میده

148
00:13:57,307 --> 00:14:01,250
درصورتی که ما اصلا بهش نگفتیم دنبال چ الگویی بگرده

149
00:14:02,596 --> 00:14:07,596
انگیزه من از طراحی این ساختار، براساس امیدی بود که داشتم

150
00:14:07,682 --> 00:14:10,023
که لایه دوم احتمالا گوشه های کوچیک رو پیدا میکنه

151
00:14:10,023 --> 00:14:14,450
و لایه سوم هم این گوشه ها رو کنار هم بزاره و حلقه ها و خط های بلندتر رو پیدا کنه

152
00:14:14,750 --> 00:14:17,479
و این تیکه ها هم کنار هم قرار بگیرن تا اون عدد رو شناسایی کنن

153
00:14:17,922 --> 00:14:21,153
خب، آیا شبکه عصبی ما هم همین کارو میکنه؟

154
00:14:21,153 --> 00:14:24,412
خب، نه اصلا

155
00:14:24,412 --> 00:14:32,250
یادته که قسمت قبل، دیدم که چطور وزن های متصل از لایه اول به هر عصب لایه دوم

156
00:14:32,250 --> 00:14:37,393
میتونه به صورت ی الگوی پیگسلی نمایش داده بشه؟ ( الگوریی که لایه دوم توی تصویر دنبالش میگرده)

157
00:14:37,393 --> 00:14:43,889
وقتی اینکارو برای وزن های لایه اول به دوم انجام میدیم

158
00:14:43,889 --> 00:14:47,408
بجای اینکه تکه های جداگونه توی تصویر عدد رو پیدا کنیم

159
00:14:47,408 --> 00:14:53,745
غیر از الگوهای ضعیفی که وسط تشکیل شده، بیشتر شانسی به نظر میرسه

160
00:14:53,745 --> 00:14:59,868
به نظر میرسه توی این فضای 13 هزار بعدی و غیرقابل تصورِ وزن ها و انحراف معیار ها

161
00:14:59,883 --> 00:15:05,248
شبکه ما ی میمیمم نسبی خوبی پیدا کرده و با اینکه بیشتر عکس هارو درست تشخیص میده

162
00:15:05,248 --> 00:15:08,899
اون الگوهایی که ما انتظارشو داشتیم پیدا نکرده

163
00:15:09,581 --> 00:15:14,173
برای اینکه درست متوجه بشی، چ اتفاقی میفته اگر ی عکس همینطوری بهش بدیم؟

164
00:15:14,173 --> 00:15:18,277
اگر باهوش باشه میشه انتظار داشته باشیم که به خروجی اش مطمئن نباشه

165
00:15:18,277 --> 00:15:23,317
یعنی هیچکدوم از عصب های لایه آخر رو روشن نکنه یا یکجور روشن کنه

166
00:15:23,317 --> 00:15:27,129
ولی با اعتماد به نفس بهت جواب اشتباه میده

167
00:15:27,129 --> 00:15:34,129
میزان اعتماد شبکه به جواب 5 اش برای این عکس الکی، به اندازه عکس خود عدد 5 عه

168
00:15:34,215 --> 00:15:38,413
به بیان دیگه، حتی اگر این شبکه بتونه اعداد رو خوب شناسایی کنه

169
00:15:38,413 --> 00:15:41,375
نمیدونه که چطوری رسم شون کنه

170
00:15:41,561 --> 00:15:45,389
بیشترش بخاطر اینه که شبکه توی فضای تمرینی سفت و سختی قرار داره

171
00:15:45,620 --> 00:15:47,831
خودتو بجای شبکه بزار

172
00:15:47,831 --> 00:15:55,109
از نظر شبکه، کل دنیا از چند تا عدد توی ی جدول تشکیل شده

173
00:15:55,109 --> 00:16:01,109
و تابع هزینه تنها چیزی که ازش میخواد اینه که به تصمیم نهایی اش مطمئن باشه

174
00:16:01,739 --> 00:16:05,541
خب این کاری بود که لایه دوم انجام میداد

175
00:16:05,541 --> 00:16:09,812
شاید واست سوال پیش بیاد که اصن من چرا امید داشتم این شبکه گوشه ها و الگو هارو پیدا کنه

176
00:16:09,961 --> 00:16:12,493
این به هیچ وجه کاری نیست که انجام میده

177
00:16:13,105 --> 00:16:17,283
خب این هدف نهایی نبود، این تازه شروع کاره

178
00:16:17,451 --> 00:16:21,653
این روش قدیمیه، کاری که توی سال های 1990 و 1980 میکردن

179
00:16:21,807 --> 00:16:26,094
و لازمه بفهمیش قبل اینکه مدل های جدید تر و دقیق تر رو بفهمی

180
00:16:26,167 --> 00:16:29,634
البته این مدل قدیمی هم میتونه مسئله های جالبی رو حل کنه

181
00:16:29,634 --> 00:16:34,649
ولی هرچی بیشتر بفهمی که این لایه های پنهان چیکار میکنن، کمتر چیز هوشمندانه ای به نظر میرسه

182
00:16:38,432 --> 00:16:42,570
ی لحظه تمرکزمون رو از اینکه چطور شبکه عصبی یادمیگره، برداریم و به این بپردازیم که "تو" چطور یادمیگیری

183
00:16:42,570 --> 00:16:46,589
این فهم فقط موقعی جا میفته که با مفهوم کار درگیر بشی

184
00:16:46,767 --> 00:16:50,740
ی کار ساده که ازتون میخوام انجام بدید اینه که ویدئو رو همین الان متوقف کنید

185
00:16:50,740 --> 00:16:55,399
و ی لحظه فکر کنید که چ تغییری میتونی توی شبکه ایجاد کنی

186
00:16:55,399 --> 00:17:01,399
اگر بخوای که گوشه ها و الگو هارو بهتر تشخیص بده؟

187
00:17:01,524 --> 00:17:09,324
یا اگر واقعا میخوای با مفهوم کار درگیر بشی، خیلی یپیشنهاد میکنم کتاب مایکل نیلسن درباره یادگیری عمیق و شبکه های عصبی رو بخونی

188
00:17:09,509 --> 00:17:14,750
توی این کتاب میتونی کد و داده هارو برای همین مثال تشخیص عدد دانلود کنی و باهاش کار کنی

189
00:17:15,101 --> 00:17:18,461
و کتاب بهت توضیح میده این کد مرحله به مرحله چیکار میکنه

190
00:17:19,082 --> 00:17:22,019
نکته خوبی که درباره این کتاب وجود داره اینه که این کتاب کاملا رایگانه و قابل دسترسه

191
00:17:22,019 --> 00:17:27,663
اگر بدردت خورد، برای زحمتی که کشیده، ی پولی هدیه بده

192
00:17:28,172 --> 00:17:31,614
لینک چند تا از منابعی که خیلی دوست داشتم رو توی قسمت توضیحات ویدئو گذاشتم

193
00:17:31,747 --> 00:17:38,247
مثل ی مقاله فوق العاده از "کریس اولا" و ی مقاله از سایت دیستیل

194
00:17:38,636 --> 00:17:43,920
برای خاتمه این قسمت، بخشی از گفتوگوی خودم با خانم "لیشا لی" رو گذاشتم

195
00:17:44,125 --> 00:17:47,855
ممکنه از ویدئو قبلی یادت باشه که ایشون پایان نامه دکترا اش رو در موضوع یادگیری عمیق نوشته

196
00:17:48,247 --> 00:17:55,747
و اینجا در مورد دو تا مقاله جدید که دقیقا چطور شبکه های عصبی تشخیص تصویر جدید یاد میگیرن صحبت میکنه

197
00:17:57,874 --> 00:18:03,375
اولین مقاله یکی از ساختار های شبکه های عمیق عصبی رو که توی تشخیص تصویر خیلی کارش خوبه رو انتخاب میکنه

198
00:18:03,375 --> 00:18:06,366
و بجای اینکه با عکس هایی که درست برچست گذاری شدن شبکه رو تمرین بده

199
00:18:06,366 --> 00:18:08,855
برچست هارو قبل از تمرین دادن بهم میریزه میکنه

200
00:18:08,855 --> 00:18:14,846
قاعدتا دقت تشخیص ی عدد شانسی درمیاد، چون برچسب های داده ها هم شانسی بوده

201
00:18:14,903 --> 00:18:21,298
ولی دقت تشخیص "داده های تمرینی" اش به اندازه شبکه عصبی بود که با داده ای که درست برچسب گذاری شده بود، تمرین داده شده

202
00:18:21,595 --> 00:18:28,095
ی جورایی میلیون ها وزن واسه این شبکه عصبی، برای حفظ کردن داده های ورودی کافی بودن

203
00:18:28,240 --> 00:18:36,625
این آزمایش، این سوال بوجود میاره که آیا کم کردن تابع هزینه، ساختار خاصی رو توی شبکه ایجاد میکنه یا فقط سعی میکنه ورودی هارو حفظ کنه؟÷

204
00:18:36,625 --> 00:18:39,971
کل داده هارو حفظ میکنه که هر داده به کدوم دسته بندی تعلق داره

205
00:18:39,971 --> 00:18:46,727
حدود نیم سال بعد، ی مقاله که دقیقا این ضد این نبود

206
00:18:46,727 --> 00:18:48,986
مقاله ای که زاویه های دیگه ای از این موضوع رو بررسی میکنه منتشر شد

207
00:18:48,986 --> 00:18:52,269
که میگفت به نظر میرسه این شبکه های عصبی کار هوشمندانه تری میکنن

208
00:18:52,269 --> 00:18:59,346
اگر به "منحنی دقت" نگاه کنی، اگر فقط روی مجموعه داده ای که به صورت شانسی برچسب گذاری شدن شبکه رو تمرین بدی

209
00:18:59,346 --> 00:19:05,352
این منحنی خیلی آروم مقدارش کم میشه - مثل ی تابع خطی

210
00:19:05,352 --> 00:19:12,499
این یعنی واقعا داری تلاش میکنی که اون نقاط مینیمم نسبی و وزن های درست رو پیدا کنی تا به دقت خوبی برسی

211
00:19:12,499 --> 00:19:16,413
ولی اگر شبکه رو با داده ای که برچسب درست داره تمرین بدی

212
00:19:16,413 --> 00:19:23,364
اولش مثل همه ولی ی دفعه مقدار تابع هزینه سقوط میکنه تا به اون دقت برسه

213
00:19:23,364 --> 00:19:29,237
ی جورایی توی داده واقعی، پیدا کردن مینیمم های نسبی راحت تره بوده

214
00:19:29,237 --> 00:19:34,366
ی چیز جالب دیگه در مورد این قضیه توی ی مقاله چند سال پیش اومده

215
00:19:34,366 --> 00:19:39,471
که خیلی ساده سازی هارو توی لایه های شبکه انجام داده

216
00:19:39,471 --> 00:19:43,788
ولی یکی نتیجه اش این بود که اگر از نظر بهینه سازی بهش نگاه کنی

217
00:19:43,788 --> 00:19:49,242
کیفیت های مینیمم های نسبی که این دوتا شبکه میخوان بهش برسن، تقریبا یکسانه

218
00:19:49,242 --> 00:19:54,568
و ی جورایی اگر داده هات برچسب درست داشته باشن، راحت تر میتونی اون مقدار های مینیمم نسبی رو پیدا کنی

219
00:19:58,692 --> 00:20:04,055
داره از اونایی که بهش پول اهدا کردن تشکر میکنه

220
00:20:04,055 --> 00:20:07,468
میگه اگر این کمک ها نبود این ویدیو رو نمیتونستم درست کنم

221
00:20:08,163 --> 00:20:41,663
دوباره داره از اون شرکته تشکر و تبلیغ میکنه

222
00:20:42,078 --> 00:20:51,188
ترجمه شده توسط انجمن علمی مهندسی کامپیوتر دانشگاه شاهد - بهمن 1400