1
00:00:04,166 --> 00:00:10,294
فرض من اینه که شما قسمت 3 رو دیده باشید. که الگوریتم "پس انتشار" رو به خوبی توضیح داده

2
00:00:11,041 --> 00:00:14,682
توی این قسمت یکم رسمی میشم و وارد ریاضی اش میشیم

3
00:00:14,800 --> 00:00:17,059
عادیه که این قسمت یکم گیج کننده باشه

4
00:00:17,062 --> 00:00:21,667
بخاطر همین ممکنه بیشتر نیاز داشته باشید ویدئو رو متوقف کنید

5
00:00:21,939 --> 00:00:28,939
هداف اصلی من اینه که بهتون نشون بدم کسایی که توی حوزه یادگیری ماشین هستن، چطور از قاعده زنجیری ریاضی در حوزه شبکه استفاده میکنن

6
00:00:29,143 --> 00:00:34,006
که ی جورایی با روشی که بیشتر آموزش های ریاضی بهش میپردازن، فرق داره

7
00:00:34,216 --> 00:00:39,225
برای کسایی که با ریاضی های مربوط به این قسمت آشنا نیستن، من کلی ویدئو در این مورد دارم

8
00:00:39,862 --> 00:00:42,938
بیایید از ی شبکه خیلی ساده شروع کنیم

9
00:00:42,939 --> 00:00:45,948
شبکه ای که هر لایه ی عصب داره

10
00:00:46,265 --> 00:00:50,539
این شبکه با 3 تا وزن و 3 تا انحراف معیار مشخص میشه

11
00:00:50,591 --> 00:00:55,234
و هدف ما اینه که مشخص کنیم تابع هزینه چقدر به این متغیر ها بستگی داره

12
00:00:55,551 --> 00:01:01,060
اینطوری مفهمیم که چ تغییری توی متغیرها، باعث سریع ترین کاهش رو توی تابع هزینه میشه

13
00:01:01,862 --> 00:01:05,311
فعلا فقط به اتصال دو تا عصب آخر میپردازیم

14
00:01:06,025 --> 00:01:09,680
مقدار فعالسازی عصب آخر رو با ... نشون میدیم

15
00:01:09,680 --> 00:01:11,311
حرف "ال" مشخص کننده لایه ایه که توش قرار داره

16
00:01:11,575 --> 00:01:15,733
پس مقدار فعالسازی عصب لایه قبل میشه ... ا

17
00:01:16,031 --> 00:01:20,022
اینا "توان" نیستن، این فقط ی روش اندیس گذاری چیزی هست که در موردش حرف میزنیم

18
00:01:20,024 --> 00:01:23,647
چون میخوام از زیرنویس (الان از بالانویس استفاده میکنه) برای کار دیگه ای استفاده کنم

19
00:01:23,706 --> 00:01:29,545
به مقدار فعالسازی آخرین عصب که برای ی داده تمرینی خاص میخوایم باشه، میگیم ... ا

20
00:01:30,010 --> 00:01:32,430
مثلا ... میتونه صفر باشه یا 1

21
00:01:32,906 --> 00:01:37,045
پس هزینه برای این شبکه ساده و ی داده تمرینی

22
00:01:37,266 --> 00:01:39,766
فرمولی که داره توضیح میده رو ببین

23
00:01:39,947 --> 00:01:44,661
به هزینه این یدونه داده تمرینی، میگم ... ا

24
00:01:46,075 --> 00:01:56,075
به عنوان یادآوری، آخرین مقدار فعالسازی برابر است با وزن ضربدر مقدار فعالسازی عصب لایه قبلی به علاوه انحراف معیار

25
00:01:57,325 --> 00:02:01,552
بعدش میندازیش توی ی تابع غیر خطی مثل سیگموید یا ریلو

26
00:02:01,554 --> 00:02:06,007
اگر به این عبارت جمع وزن دار ی اسم جدا بدیم، کار باهاش راحت تر میشه

27
00:02:06,009 --> 00:02:09,770
مثل ... با ی بالانویس درست مثل بالانویس مقدار فعالساز مربوطه اش

28
00:02:10,104 --> 00:02:11,479
مقدار علامت ها و نماد ها زیاد شد

29
00:02:11,479 --> 00:02:13,586
ی روشی که باهاش بتونی این عبارات رو تصور کنی اینه که

30
00:02:13,590 --> 00:02:18,966
وزن و مقدار فعالسازی عصب قبلی و انحراف معیار همه باهم استفاده میشن تا مقدار ... حساب بشه

31
00:02:19,180 --> 00:02:21,568
که باعث میشه بتونیم .... رو حساب کنیم

32
00:02:21,568 --> 00:02:25,639
که با ثابت .... میتونیم هزینه رو حساب کنیم

33
00:02:26,937 --> 00:02:32,127
و این قضیه روی عصب قبلی هم برقراره

34
00:02:32,668 --> 00:02:34,991
ولی فعلا بهش نمیپردازیم

35
00:02:35,729 --> 00:02:41,747
همشون فقط ی عدد هستن، درسته؟ خوبه که واسه همشون ی محور اعداد در نظر بگیریم

36
00:02:41,955 --> 00:02:49,157
اولین هدفمون اینه که بفهمیم تابع هزینه مون چقدر به تغییرات کوچیک تو وزن حساسه

37
00:02:49,794 --> 00:02:54,949
یا به بیان دیگه، مقدار مشتق هزینه به وزن چقدره

38
00:02:55,566 --> 00:03:02,733
وقتی علامت دلتا رو کنار وزن میبینی، میتونی منظورش رو ی تغییر خیلی کوچیک مثل یک صدم در توی وزن نظر بگیری

39
00:03:03,020 --> 00:03:08,091
و ... رو میتونی تاثیر روی هزینه برداشت کنی

40
00:03:08,336 --> 00:03:10,518
ما نسبت این مقدار رو میخوایم

41
00:03:10,675 --> 00:03:16,485
به طور مفهومی، این تغییر کوچیک توی وزن، باعث ی تغییری توی جمع وزن دارشون میشه

42
00:03:16,489 --> 00:03:21,514
که بعد باعث تغییر ... میشه و مستقیم روی هزینه تاثیر میزاره

43
00:03:23,126 --> 00:03:29,306
خب اول با توجه به تغییرات جمع وزن دار به تغییرات وزن، مسئله رو به اجزای کوچک تر تقسیم میکنیم

44
00:03:29,491 --> 00:03:33,079
که میشه مشتق جمع وزن دار به وزن

45
00:03:33,735 --> 00:03:39,735
به همین ترتیب، تغییرات ... نسبت به تغییر باعث شده روی جمع وزن دار رو در نظر میگیریم

46
00:03:39,735 --> 00:03:45,235
و بعد تغییرات هزینه نسبت به ... ا

47
00:03:45,758 --> 00:03:47,693
این قاعده زنجیره ای بود

48
00:03:47,697 --> 00:03:55,044
این ضرب این 3 تا نسبت بهمون مقدار تغییر تابع هزینه نسبت به وزن رو میده

49
00:03:57,044 --> 00:04:03,210
الان توی صفحه کلی علامت و نماد ریاضیه، مطمئن شو که الان همه شو میفهمی

50
00:04:03,322 --> 00:04:06,747
الان میخوایم مشتق های مرتبط رو حساب کنیم

51
00:04:07,012 --> 00:04:13,012
نگاه کن ببین چیکار میکنه

52
00:04:13,704 --> 00:04:21,204
توجه کن که این یعنی اندازه اش، با مقدار خروجی و چیزی که میخوایم خروجی بشه متناسبه

53
00:04:21,362 --> 00:04:27,341
این یعنی خروجی خیلی فرق داشت، تغییرات کوچیک هم میتونه تاثیر زیادی روی تابع هزینه داشته باشه

54
00:04:27,836 --> 00:04:44,836
به توضیحات توجه کن

55
00:04:45,898 --> 00:04:53,898
در مورد شما نمیدونم ولی به نظر من اگر ی لحظه به خودت استراحت ندی و اینا رو ی مرور نکنی، توش هنگ میکنی

56
00:04:54,076 --> 00:04:59,862
توی مشتق آخر، مقداری که تغییر توی وزن باعث تغییر توی عصب بعدش میشه

57
00:04:59,862 --> 00:05:02,862
بستگی به این داره که عصب قبلش چقدر فعاله

58
00:05:02,935 --> 00:05:08,435
یادتون باشه اینجا اشاره ای به نظریه "عصب هایی که باهم فعال میشن، بهم متصل هستن" داره

59
00:05:09,045 --> 00:05:16,045
همه اینا مشتق برحسب وزن برای هزینه براساس یک داده تمرینی خاص هست

60
00:05:16,456 --> 00:05:21,956
از اونجایی که برای تابع هزینه کامل، باید از میانگین هزینه همه داده تمرینی میانگین بگیری

61
00:05:22,052 --> 00:05:28,052
برای محاسبه مشتقش هم باید از همه داده های تمرینی با این فرمولی که بدست اوردم، میانگین بگیری

62
00:05:28,480 --> 00:05:31,884
که البته یک جزء بردار گرادیان هست

63
00:05:31,886 --> 00:05:38,495
که خودش از مشتق های جزئی تابع هزینه نسبت به همه وزن ها و انحراف معیار ها ساخته میشه

64
00:05:40,619 --> 00:05:45,716
اگرچه این یکی از مشتق هایی هست که لازم داشتیم، ولی بیشتر از نصف کارو جلو اومدیم

65
00:05:46,403 --> 00:05:50,156
فرمول بر اساس انحراف معیار، تقریبا یکیه

66
00:05:50,156 --> 00:05:56,129
فقط لازمه که .... با  .... عوض کنیم

67
00:05:58,438 --> 00:06:04,083
و اگر به فرمول نگاه کنی، جواب این مشتق میشه 1

68
00:06:06,024 --> 00:06:10,153
اینجا ایده بازگشت به عقب مطرح میشه

69
00:06:10,153 --> 00:06:16,153
میتونی ببینی که حساسیت این تابع به مقدار فعالسازی لایه قبل چقدره

70
00:06:16,156 --> 00:06:19,406
به بیان دیگه، این مشتق اول توی عبارت زنجیری

71
00:06:19,406 --> 00:06:26,406
حساسیت جمع وزن دار به مقدار فعالسازی لایه قبل، میشه وزن اش

72
00:06:26,564 --> 00:06:32,870
حتی اگر نتونیم مستقیم تاثیر لایه قبل رو بفهمیم، خوبه که بتونیم رصدش کنیم

73
00:06:32,992 --> 00:06:42,908
بخاطر اینکه الان میتونیم این قاعده رنجیری به سمت عقب اعمال کنیم تا بفهمیم تابع هزینه چقدر به وزن ها و انحراف معیار های لایه قبل حساسیت داره

74
00:06:43,410 --> 00:06:47,870
شاید فکر کنی که مثال بیش از حد ساده ای بود بخاطر اینکه هر لایه فقط ی عصب داشت

75
00:06:47,870 --> 00:06:51,437
و در عمل پیچیدگی کار به صورت نمایی بیشتر میشه

76
00:06:51,439 --> 00:06:56,052
ولی انقدا هم فرق نمیکنه وقتی لایه ها بیشتر عصب دارن

77
00:06:56,055 --> 00:06:58,942
فقط چند تا اندیس بیشتر رو باید در نظر بگیریم

78
00:06:58,942 --> 00:07:07,274
بجای اینکه عدد فعالسازی یک لایه فقط باشه ... ، زیرنویس هم میگیره که مشخص کننده اینه که کدوم عصب از لایه است

79
00:07:07,514 --> 00:07:11,802
برای اندیس عصب های لایه قبل از ... استفاده میکنیم

80
00:07:11,802 --> 00:07:15,122
و .... برای عصب های لایه حاضر

81
00:07:15,329 --> 00:07:18,733
برای هزینه، ما نگاه میکنیم که مقدار مورد انتظار ما چیه

82
00:07:18,735 --> 00:07:25,653
ولی ایندفعه مربع های تفاوت بین مقدار فعالسازی دلخواه و خروجی رو باهم جمع میکنیم

83
00:07:26,236 --> 00:07:32,236
عبارت توی صفحه رو نگاه کن

84
00:07:33,016 --> 00:07:37,959
از اونجایی که وزن های بیشتری داریم، برای مشخص شدن اینکه هرکدوم کجان باید چند تا اندیس دیگه اضافه کنیم

85
00:07:38,115 --> 00:07:45,115
به اتصالی که عصب ... رو به عصب ... وصل میکنه میگیم .... ا

86
00:07:45,346 --> 00:07:53,346
شاید این اندیس ها برعکس به نظر برسن ولی همونطوریه که باید وزن هارو بنویسی که توی قسمت اول توضیح دادم

87
00:07:53,675 --> 00:07:57,355
مثل قبل، خوبه که به جمع وزن دار مربوط ی اسم خاص بدیم

88
00:07:57,357 --> 00:08:04,334
مثلا ... ، که در صورت مقدار فعالسازی لایه آخر ی تابع خاصه مثل سیگموید که ورودیش ... هست

89
00:08:04,810 --> 00:08:06,591
الان میتونی بقهمی که من تقریبا منظورم چیه درسته؟

90
00:08:06,593 --> 00:08:11,324
همه اینا تقریبا همون معادله هایین که توی مثال "یک عصب در لایه" داشتیم

91
00:08:11,324 --> 00:08:14,677
ولی یکم پیچیده تر به نظر میرسه

92
00:08:15,249 --> 00:08:21,838
عبارت مشتق زنجیری ای که مشخص میکنه که حساسیت هزینه نسبت به وزن خاص چقدره

93
00:08:21,838 --> 00:08:23,774
در اصل یک جور به نظر میرسه

94
00:08:23,774 --> 00:08:27,194
یکم تنهات میزارم اگر میخوای روش فکر کنی

95
00:08:29,307 --> 00:08:36,957
چیزی که اینجا تغییر کرده، مشتق هزینه نسبت به مقدار فعالسازی یکی از عصب ها توی لایه قبله

96
00:08:37,615 --> 00:08:42,879
اینجا فرق اینه که عصب باعث تغییر تابع هزینه در چند مسیر متفاوت میشه

97
00:08:44,556 --> 00:08:50,863
از ی طرف روی ... رو تاثیر میزاره، که جزئی از تابع هزینه است

98
00:08:50,864 --> 00:08:56,149
ولی روی ... هم تاثیر میزاره که اونم جزء تابع هزینه است

99
00:08:56,152 --> 00:08:57,801
و باید جمعشون کنی

100
00:08:59,916 --> 00:09:03,341
و خب، تقریبا همه اش همین بود

101
00:09:03,341 --> 00:09:08,557
وقتی که فهمیدی حساسیت به مقدار فعالسازی لایه یکی به آخر چجوری بدست میاد

102
00:09:08,671 --> 00:09:13,037
میتونی این روند رو برای همه لایه هایی که به این لایه متصل ان تکرار کنی

103
00:09:13,769 --> 00:09:15,173
به خودت افتخار کن

104
00:09:15,173 --> 00:09:20,172
اگر همه اینا رو فهمیدی، تو ی نگاه به اعماق الگوریتم "پس انتشار" انداختی

105
00:09:20,173 --> 00:09:23,167
الگوریتم پایه ای و پشت پرده یادگیری شبکه عصبی

106
00:09:23,625 --> 00:09:28,985
این عبارت های زنجیری، بهت مشتقی میده که هر جزء این گرادیان رو بهت میده

107
00:09:28,985 --> 00:09:33,687
گرادیانی اگر به صورت پیوسته محاسبه اش کنی، بهت کمک میکنه تا مقدار تابع هزینه رو به کمترین مقدار برسونی

108
00:09:34,042 --> 00:09:39,969
پوفففف، اگر ی لحظه به صندلی تکیه بدی و بهش فکر کنی، متوجه میشی که کلی پیچیدگی داره تا بهش احاطه پیدا کنی

109
00:09:40,581 --> 00:09:44,820
پس اگر یکم زمان برد تا این مطالب جا بیفته برات، خیلی نگران نباش

110
00:09:46,649 --> 00:10:16,649
**انجمن علمی مهندسی کامپیوتر دانشگاه شاهد - بهمن 1400**